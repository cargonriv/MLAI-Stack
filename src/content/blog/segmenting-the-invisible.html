<!DOCTYPE html>
<html>
  <head>
    <title>Segmenting the Invisible</title>
    <meta content="text/html; charset=UTF-8" http-equiv="content-type" />
    <style type="text/css">
      .lst-kix_ehzt5islygnw-3 > li:before {
        content: "\0025cf   ";
      }
      .lst-kix_pq6l9j27g4h8-7 > li:before {
        content: "\0025cb   ";
      }
      .lst-kix_ehzt5islygnw-2 > li:before {
        content: "\0025a0   ";
      }
      .lst-kix_ehzt5islygnw-4 > li:before {
        content: "\0025cb   ";
      }
      .lst-kix_pq6l9j27g4h8-5 > li:before {
        content: "\0025a0   ";
      }
      .lst-kix_pq6l9j27g4h8-4 > li:before {
        content: "\0025cb   ";
      }
      .lst-kix_pq6l9j27g4h8-8 > li:before {
        content: "\0025a0   ";
      }
      .lst-kix_pq6l9j27g4h8-3 > li:before {
        content: "\0025cf   ";
      }
      .lst-kix_ehzt5islygnw-6 > li:before {
        content: "\0025cf   ";
      }
      .lst-kix_ehzt5islygnw-5 > li:before {
        content: "\0025a0   ";
      }
      .lst-kix_pq6l9j27g4h8-2 > li:before {
        content: "\0025a0   ";
      }
      .lst-kix_pq6l9j27g4h8-1 > li:before {
        content: "\0025cb   ";
      }
      .lst-kix_pq6l9j27g4h8-0 > li:before {
        content: "\0025a0   ";
      }
      .lst-kix_ajkvb22l6j96-0 > li:before {
        content: "\0025a0   ";
      }
      .lst-kix_ehzt5islygnw-0 > li:before {
        content: "\0025a0   ";
      }
      .lst-kix_ehzt5islygnw-1 > li:before {
        content: "\0025cb   ";
      }
      .lst-kix_ikcljutuvuve-8 > li:before {
        content: "\0025a0   ";
      }
      ul.lst-kix_ehzt5islygnw-8 {
        list-style-type: none;
      }
      ul.lst-kix_ehzt5islygnw-5 {
        list-style-type: none;
      }
      .lst-kix_ikcljutuvuve-6 > li:before {
        content: "\0025cf   ";
      }
      .lst-kix_ajkvb22l6j96-1 > li:before {
        content: "\0025cb   ";
      }
      ul.lst-kix_ehzt5islygnw-4 {
        list-style-type: none;
      }
      ul.lst-kix_ehzt5islygnw-7 {
        list-style-type: none;
      }
      .lst-kix_ikcljutuvuve-5 > li:before {
        content: "\0025a0   ";
      }
      .lst-kix_ikcljutuvuve-7 > li:before {
        content: "\0025cb   ";
      }
      ul.lst-kix_ehzt5islygnw-6 {
        list-style-type: none;
      }
      ul.lst-kix_ehzt5islygnw-1 {
        list-style-type: none;
      }
      .lst-kix_ajkvb22l6j96-2 > li:before {
        content: "\0025a0   ";
      }
      .lst-kix_ajkvb22l6j96-3 > li:before {
        content: "\0025cf   ";
      }
      ul.lst-kix_ehzt5islygnw-0 {
        list-style-type: none;
      }
      ul.lst-kix_ajkvb22l6j96-1 {
        list-style-type: none;
      }
      ul.lst-kix_ehzt5islygnw-3 {
        list-style-type: none;
      }
      ul.lst-kix_ajkvb22l6j96-0 {
        list-style-type: none;
      }
      ul.lst-kix_ehzt5islygnw-2 {
        list-style-type: none;
      }
      ul.lst-kix_ajkvb22l6j96-3 {
        list-style-type: none;
      }
      ul.lst-kix_ajkvb22l6j96-2 {
        list-style-type: none;
      }
      .lst-kix_ajkvb22l6j96-4 > li:before {
        content: "\0025cb   ";
      }
      .lst-kix_ajkvb22l6j96-5 > li:before {
        content: "\0025a0   ";
      }
      .lst-kix_ikcljutuvuve-2 > li:before {
        content: "\0025a0   ";
      }
      ul.lst-kix_ajkvb22l6j96-5 {
        list-style-type: none;
      }
      .lst-kix_ikcljutuvuve-3 > li:before {
        content: "\0025cf   ";
      }
      ul.lst-kix_ikcljutuvuve-8 {
        list-style-type: none;
      }
      ul.lst-kix_ajkvb22l6j96-4 {
        list-style-type: none;
      }
      ul.lst-kix_ajkvb22l6j96-7 {
        list-style-type: none;
      }
      .lst-kix_ikcljutuvuve-4 > li:before {
        content: "\0025cb   ";
      }
      ul.lst-kix_ikcljutuvuve-6 {
        list-style-type: none;
      }
      ul.lst-kix_ajkvb22l6j96-6 {
        list-style-type: none;
      }
      .lst-kix_ikcljutuvuve-0 > li:before {
        content: "\0025a0   ";
      }
      .lst-kix_ikcljutuvuve-1 > li:before {
        content: "\0025cb   ";
      }
      ul.lst-kix_ikcljutuvuve-7 {
        list-style-type: none;
      }
      ul.lst-kix_ikcljutuvuve-4 {
        list-style-type: none;
      }
      ul.lst-kix_ajkvb22l6j96-8 {
        list-style-type: none;
      }
      ul.lst-kix_ikcljutuvuve-5 {
        list-style-type: none;
      }
      .lst-kix_ehzt5islygnw-7 > li:before {
        content: "\0025cb   ";
      }
      ul.lst-kix_ikcljutuvuve-2 {
        list-style-type: none;
      }
      .lst-kix_ajkvb22l6j96-8 > li:before {
        content: "\0025a0   ";
      }
      ul.lst-kix_ikcljutuvuve-3 {
        list-style-type: none;
      }
      .lst-kix_ehzt5islygnw-8 > li:before {
        content: "\0025a0   ";
      }
      ul.lst-kix_ikcljutuvuve-0 {
        list-style-type: none;
      }
      ul.lst-kix_ikcljutuvuve-1 {
        list-style-type: none;
      }
      .lst-kix_ajkvb22l6j96-6 > li:before {
        content: "\0025cf   ";
      }
      .lst-kix_ajkvb22l6j96-7 > li:before {
        content: "\0025cb   ";
      }
      ul.lst-kix_thjubi60nqtj-3 {
        list-style-type: none;
      }
      ul.lst-kix_thjubi60nqtj-2 {
        list-style-type: none;
      }
      ul.lst-kix_thjubi60nqtj-1 {
        list-style-type: none;
      }
      ul.lst-kix_thjubi60nqtj-0 {
        list-style-type: none;
      }
      ul.lst-kix_thjubi60nqtj-7 {
        list-style-type: none;
      }
      ul.lst-kix_thjubi60nqtj-6 {
        list-style-type: none;
      }
      ul.lst-kix_thjubi60nqtj-5 {
        list-style-type: none;
      }
      ul.lst-kix_thjubi60nqtj-4 {
        list-style-type: none;
      }
      .lst-kix_thjubi60nqtj-1 > li:before {
        content: "\0025cb   ";
      }
      .lst-kix_thjubi60nqtj-2 > li:before {
        content: "\0025a0   ";
      }
      ul.lst-kix_pq6l9j27g4h8-3 {
        list-style-type: none;
      }
      ul.lst-kix_pq6l9j27g4h8-4 {
        list-style-type: none;
      }
      ul.lst-kix_pq6l9j27g4h8-5 {
        list-style-type: none;
      }
      ul.lst-kix_thjubi60nqtj-8 {
        list-style-type: none;
      }
      ul.lst-kix_pq6l9j27g4h8-6 {
        list-style-type: none;
      }
      .lst-kix_thjubi60nqtj-7 > li:before {
        content: "\0025cb   ";
      }
      ul.lst-kix_pq6l9j27g4h8-0 {
        list-style-type: none;
      }
      li.li-bullet-0:before {
        margin-left: -18pt;
        white-space: nowrap;
        display: inline-block;
        min-width: 18pt;
      }
      .lst-kix_thjubi60nqtj-0 > li:before {
        content: "\0025a0   ";
      }
      .lst-kix_thjubi60nqtj-8 > li:before {
        content: "\0025a0   ";
      }
      ul.lst-kix_pq6l9j27g4h8-1 {
        list-style-type: none;
      }
      ul.lst-kix_pq6l9j27g4h8-2 {
        list-style-type: none;
      }
      ul.lst-kix_pq6l9j27g4h8-7 {
        list-style-type: none;
      }
      ul.lst-kix_pq6l9j27g4h8-8 {
        list-style-type: none;
      }
      .lst-kix_thjubi60nqtj-6 > li:before {
        content: "\0025cf   ";
      }
      .lst-kix_thjubi60nqtj-5 > li:before {
        content: "\0025a0   ";
      }
      .lst-kix_thjubi60nqtj-3 > li:before {
        content: "\0025cf   ";
      }
      .lst-kix_thjubi60nqtj-4 > li:before {
        content: "\0025cb   ";
      }
      .lst-kix_pq6l9j27g4h8-6 > li:before {
        content: "\0025cf   ";
      }
      ol {
        margin: 0;
        padding: 0;
      }
      table td,
      table th {
        padding: 0;
      }
      .c8 {
        border-right-style: solid;
        padding-top: 0pt;
        border-top-width: 0pt;
        border-right-width: 0pt;
        padding-left: 0pt;
        padding-bottom: 0pt;
        line-height: 1.38;
        border-left-width: 0pt;
        border-top-style: solid;
        border-left-style: solid;
        border-bottom-width: 0pt;
        border-bottom-style: solid;
        orphans: 2;
        widows: 2;
        text-align: left;
        padding-right: 0pt;
      }
      .c4 {
        padding-top: 0pt;
        border-top-width: 0pt;
        padding-bottom: 0pt;
        line-height: 1.56;
        border-top-style: solid;
        margin-left: 51pt;
        text-indent: -18pt;
        border-bottom-width: 0pt;
        border-bottom-style: solid;
        orphans: 2;
        widows: 2;
        text-align: left;
        height: 11pt;
      }
      .c6 {
        padding-top: 0pt;
        border-top-width: 0pt;
        padding-left: 0pt;
        padding-bottom: 0pt;
        line-height: 1.56;
        border-top-style: solid;
        margin-left: 51pt;
        border-bottom-width: 0pt;
        border-bottom-style: solid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      .c13 {
        padding-top: 0pt;
        border-top-width: 0pt;
        padding-bottom: 0pt;
        line-height: 1.56;
        border-top-style: solid;
        border-bottom-width: 0pt;
        border-bottom-style: solid;
        orphans: 2;
        widows: 2;
        text-align: left;
        height: 11pt;
      }
      .c17 {
        padding-top: 0pt;
        border-top-width: 0pt;
        border-bottom-width: 0pt;
        padding-bottom: 0pt;
        line-height: 1.38;
        border-bottom-style: solid;
        orphans: 2;
        border-top-style: solid;
        widows: 2;
        text-align: left;
      }
      .c11 {
        padding-top: 0pt;
        border-top-width: 0pt;
        border-bottom-width: 0pt;
        padding-bottom: 13.5pt;
        line-height: 1.26;
        border-bottom-style: solid;
        orphans: 2;
        border-top-style: solid;
        widows: 2;
        text-align: center;
      }
      .c12 {
        padding-top: 0pt;
        border-top-width: 0pt;
        border-bottom-width: 0pt;
        padding-bottom: 0pt;
        line-height: 1.56;
        border-bottom-style: solid;
        orphans: 2;
        border-top-style: solid;
        widows: 2;
        text-align: center;
      }
      .c18 {
        padding-top: 0pt;
        padding-bottom: 0pt;
        line-height: 1.15;
        orphans: 2;
        widows: 2;
        text-align: left;
        height: 11pt;
      }
      .c15 {
        font-weight: 400;
        text-decoration: none;
        vertical-align: baseline;
        font-size: 26pt;
        font-family: "Arial";
        font-style: normal;
      }
      .c9 {
        font-weight: 400;
        text-decoration: none;
        vertical-align: baseline;
        font-size: 17pt;
        font-family: "Arial";
        font-style: normal;
      }
      .c3 {
        font-weight: 400;
        text-decoration: none;
        vertical-align: baseline;
        font-size: 11pt;
        font-family: "Arial";
        font-style: normal;
      }
      .c2 {
        font-weight: 400;
        text-decoration: none;
        vertical-align: baseline;
        font-size: 12pt;
        font-family: "Arial";
        font-style: normal;
      }
      .c10 {
        font-weight: 400;
        text-decoration: none;
        vertical-align: baseline;
        font-size: 11pt;
        font-family: "Arial";
      }
      .c5 {
        background-color: #ffffff;
        max-width: 468pt;
        padding: 72pt 72pt 72pt 72pt;
      }
      .c0 {
        color: #0e101a;
        font-style: italic;
      }
      .c16 {
        padding: 0;
        margin: 0;
      }
      .c1 {
        color: #0e101a;
      }
      .c7 {
        font-weight: 700;
      }
      .c14 {
        color: #000000;
      }
      .title {
        padding-top: 0pt;
        color: #000000;
        font-size: 26pt;
        padding-bottom: 3pt;
        font-family: "Arial";
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      .subtitle {
        padding-top: 0pt;
        color: #666666;
        font-size: 15pt;
        padding-bottom: 16pt;
        font-family: "Arial";
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      li {
        color: #000000;
        font-size: 11pt;
        font-family: "Arial";
      }
      p {
        margin: 0;
        color: #000000;
        font-size: 11pt;
        font-family: "Arial";
      }
      h1 {
        padding-top: 20pt;
        color: #000000;
        font-size: 20pt;
        padding-bottom: 6pt;
        font-family: "Arial";
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      h2 {
        padding-top: 18pt;
        color: #ffffff;
        font-size: 16pt;
        padding-bottom: 6pt;
        font-family: "Arial";
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      h3 {
        padding-top: 16pt;
        color: #434343;
        font-size: 14pt;
        padding-bottom: 4pt;
        font-family: "Arial";
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      h4 {
        padding-top: 14pt;
        color: #666666;
        font-size: 12pt;
        padding-bottom: 4pt;
        font-family: "Arial";
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      h5 {
        padding-top: 12pt;
        color: #666666;
        font-size: 11pt;
        padding-bottom: 4pt;
        font-family: "Arial";
        line-height: 1.15;
        page-break-after: avoid;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
      h6 {
        padding-top: 12pt;
        color: #666666;
        font-size: 11pt;
        padding-bottom: 4pt;
        font-family: "Arial";
        line-height: 1.15;
        page-break-after: avoid;
        font-style: italic;
        orphans: 2;
        widows: 2;
        text-align: left;
      }
    </style>
  <style>
    body.c5 {
        background-color: #0e101a;
    }
    .title, .subtitle, h1, h2, h3, h4, h5, h6, p, li, span {
        color: #ffffff !important;
    }
    .title {
        font-size: 36pt !important;
        text-align: center !important;
    }
    .subtitle {
        font-size: 24pt !important;
        text-align: center !important;
    }
    h1 { font-size: 30pt !important; }
    h2 { font-size: 24pt !important; }
    h3 { font-size: 20pt !important; }
    h4 { font-size: 18pt !important; }
    h5 { font-size: 16pt !important; }
    h6 { font-size: 14pt !important; }
    p {
        margin-bottom: 1em;
    }
</style>
</head>
  <body class="c5 doc-content">
    <h1 class="c11 title" id="h.iq1b57b6irdh">
      <span class="c14 c15">Segmenting the Invisible</span>
    </h1>
    <h2 class="c12 subtitle" id="h.6x6u16m6167t">
      <span>SAM, DINO, and the Future of Image Analysis</span>
    </h2>
    <h3 class="c17" id="h.1sob6urgak60">
      Introduction<span class="c9 c1"></span>
    </h2>
    <p class="c8">
      <span class="c1"
        >In biological imaging, we often deal with the &quot;invisible&quot;:
        microscopic cells, bacterial colonies, or subtle patterns that evade
        easy detection. Traditional image analysis required painstaking tuning
        of algorithms or training models from scratch on limited data. Today, a
        new wave of </span
      ><span class="c1 c7">foundation models</span
      ><span class="c1">&nbsp;promises to change that. Meta&#39;s </span
      ><span class="c1 c7">Segment Anything Model (SAM)</span
      ><span class="c1"
        >&nbsp;and related vision transformers, abbreviated as the ViT term like </span
      ><span class="c1 c7">DINO</span
      ><span class="c1"
        >&nbsp;(self-distillation with no labels), are generalist vision models
        trained on massive data. They can </span
      ><span class="c1 c7">zero-shot segment or describe objects</span
      ><span class="c1"
        >&nbsp;in images without prior task-specific training. Even more
        exciting, </span
      ><span class="c1 c7">Grounding DINO</span
      ><span class="c1"
        >&nbsp;extends this capability to open-vocabulary object detection by
        finding objects in an image based on </span
      ><span class="c1 c7">text prompts</span
      ><span class="c1"
        >. These advances foreshadow a future of bioimage analysis where AI can </span
      ><span class="c0">segment anything</span
      ><span class="c3 c1"
        >&nbsp;we need, even in complex experimental contexts, with minimal
        human supervision.</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <p class="c8">
      <span class="c1">But, how well do these tools work on </span
      ><span class="c1 c7">biological</span
      ><span class="c3 c1"
        >&nbsp;images, and how are labs leveraging them? In this post, we
        explore recent applications of SAM and DINO in biology, from microscopic
        cell imaging to high-throughput plate assays, and how pairing them with
        clever filtering and domain knowledge can unlock new workflows.
        We&#39;ll also highlight case studies (academic and industry) and even
        some of my projects using these models, giving a glimpse into the future
        of automated bioimage analysis.</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <h3 class="c8" id="h.7jghb41kqah">
      <span class="c9 c1">SAM: A Foundation Model for Segmentation</span>
    </h3>
    <p class="c8">
      <span class="c1"
        >Meta&#39;s AI team introduced SAM in 2023 as a general </span
      ><span class="c1 c7">promptable segmentation</span
      ><span class="c1"
        >&nbsp;model that can delineate any object in an image given minimal
        prompts (points, boxes, etc.). Trained on over a billion masks, SAM
        boasts broad generalization. Researchers wasted no time testing SAM on
        biomedical data. Early studies show a mix of promise and limitations:
        SAM achieved </span
      ><span class="c1 c7"
        >impressive zero-shot segmentation on some medical images</span
      ><span class="c3 c1"
        >&nbsp;but struggled on others without fine-tuning. For example,
        out-of-the-box SAM can outline large, high-contrast structures (like
        organs or colonies) with minimal input, but for subtle features (faint
        cell boundaries, noisy microscopies), its performance drops.</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <p class="c8">
      <span class="c1 c7">Adapting SAM to specific domains</span
      ><span class="c1"
        >&nbsp;has been a key focus in 2024&ndash;2025. Liu </span
      ><span class="c0">et al.</span><span class="c1">&nbsp;introduced </span
      ><span class="c1 c7">MedSAM</span
      ><span class="c1"
        >, fine-tuning SAM for medical imaging to bridge the gap between natural
        and medical domains. This year, a team led by Pape </span
      ><span class="c0">et al.</span><span class="c1">&nbsp;released </span
      ><span class="c1 c7">Segment Anything for Microscopy (&mu;SAM)</span
      ><span class="c1"
        >, which fine-tuned SAM on light and electron microscopy data. The
        result was significantly improved segmentation quality on cell and
        tissue images, compared to vanilla SAM. &mu;SAM even comes as a Napari
        plugin for interactive segmentation and tracking, offering biologists a
        unified, user-friendly tool for various microscopy modalities. These
        efforts demonstrate that we can harness SAM&#39;s </span
      ><span class="c1 c7">foundation model</span
      ><span class="c3 c1"
        >&nbsp;knowledge with modest domain-specific tuning for accurate
        segmentation of biological structures that were previously
        &quot;invisible&quot; to generic models.</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <p class="c8">
      <span class="c1">It&#39;s worth noting that SAM is primarily an </span
      ><span class="c1 c7">instance segmentation</span
      ><span class="c1"
        >&nbsp;model that finds object masks but doesn&#39;t label what they
        are. In biology, we often care about </span
      ><span class="c0">which</span
      ><span class="c3 c1"
        >&nbsp;cell type or colony a segment is. This need is where models like
        DINO and Grounding DINO come in (more on that below). First, let&#39;s
        see SAM in action in a classic microbiology problem: bacterial colony
        counting.</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <h2 class="c8" id="h.w9pnya87hk8d">
      <span class="c9 c1"
        >Case Study: Bacterial Colony Segmentation on Agar Plates</span
      >
    </h2>
    <p class="c8">
      <span class="c1"
        >Counting and analyzing bacterial colonies on agar plates is a
        fundamental task in microbiolog that has historically required either
        manual counting or training specialized models (e.g., U-Nets or Mask
        R-CNNs) for segmentation. With SAM, we now have a ready-made model that
        can </span
      ><span class="c1 c7">segment colonies without any training</span
      ><span class="c1"
        >&nbsp;in specific microbiology images. Researchers have begun exploring
        this. Ili&#263; </span
      ><span class="c0">et al.</span
      ><span class="c1">&nbsp;tested SAM on the </span
      ><span class="c1 c7">AGAR dataset</span
      ><span class="c1"
        >&nbsp;(18,000 images of Petri dishes with various bacteria species) and
        found that SAM could indeed detect and mask most colonies in an image
        zero-shot. In one example, SAM produced around </span
      ><span class="c1 c7">190 segmentation masks</span
      ><span class="c3 c1"
        >&nbsp;on a single petri dish image! This segmentation effectively
        outlined each visible colony in different colors. The model even outputs
        metadata (e.g., bounding boxes, mask area) for each object, which we can
        export as a CSV for analysis. This showcase proved that, even without
        training, a general model like SAM can handle dense microbial images and
        pull out individual colony regions.</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <p class="c8">
      <span class="c1"
        >That said, SAM isn&#39;t perfect on these images. It may produce some
        spurious or partial masks (e.g., fragmenting one colony into multiple
        pieces or merging neighboring colonies into one mask). An </span
      ><span class="c1 c7">important step is filtering SAM&#39;s output</span
      ><span class="c1"
        >&nbsp;to discard poor segments. In their colony analysis pipeline,
        Sidiropoulos </span
      ><span class="c0">et al.</span><span class="c1">&nbsp;used a </span
      ><span class="c0">pre-trained SAM (frozen)</span
      ><span class="c1">&nbsp;to cut out colonies and then </span
      ><span class="c1 c7">filtered out bad masks to avoid artifacts</span
      ><span class="c1"
        >. What counts as a &quot;bad&quot; mask? These are often irregular
        shapes or blurry segments that don&#39;t correspond to a single colony.
        For instance, SAM might grab a piece of writing on the plate or a colony
        cluster as one mask. By filtering based on properties like mask </span
      ><span class="c1 c7">area, circularity, and solidity</span
      ><span class="c3 c1"
        >, one can keep only nicely rounded, reasonably sized masks (likely
        single colonies). In the mentioned study, they explicitly removed SAM
        masks that looked erroneous before using the rest for data
        augmentation.</span
      >
    </p>
    <p class="c13"><span class="c3 c1"></span></p>
    <p class="c8">
      <span class="c0"
        >Figure: Examples of SAM&#39;s colony segmentation on an agar plate,
        highlighting the need for filtering. Top: </span
      ><span class="c0 c7">Good segmentations</span
      ><span class="c0"
        >&nbsp;&ndash; SAM masks cleanly capturing individual bacterial colonies
        (mostly circular). Bottom: </span
      ><span class="c0 c7">Bad segmentations</span
      ><span class="c0 c10"
        >&nbsp;&ndash; SAM masks that are incomplete, merged, or otherwise
        inaccurate. By filtering out irregular masks (e.g., non-circular shapes
        or fragments), one can automatically focus on the correct colony
        segments.</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <p class="c8">
      <span class="c1">Our own experience aligns with this: using </span
      ><span class="c1 c7">SAM + circularity filtering</span
      ><span class="c3 c1"
        >&nbsp;on multi-well plate images, we could automatically pick out the
        top candidate mask for each well (usually the colony or region of
        interest) while ignoring debris or artifacts. This technique drastically
        reduces false positives in high-throughput assays. In essence, SAM
        provides the initial &quot;guess&quot; for every object, and a simple
        rule-based filter (domain knowledge like &quot;colonies are round&quot;)
        refines those guesses. It&#39;s a powerful combo of a general model with
        a domain-specific heuristic.</span
      >
    </p>
    <p class="c4"><span class="c1 c2"></span></p>
    <h2 class="c8" id="h.4vj9v1n3kemz">
      <span class="c9 c1">Synthetic Data Augmentation with SAM</span>
    </h2>
    <p class="c8">
      <span class="c1"
        >One remarkable case study that ties together SAM and downstream
        detection is a recent pipeline for colony counting. Instead of training
        a colony detector on limited real images, the authors used SAM to </span
      ><span class="c1 c7">generate synthetic training data</span
      ><span class="c1"
        >. As illustrated in the figure below, the process was: take a handful
        of real plate images &rarr; use SAM to segment every colony &rarr;
        filter out the good colony masks &rarr; </span
      ><span class="c1 c7">copy-paste</span
      ><span class="c1"
        >&nbsp;those colony cutouts onto blank agar backgrounds to make new
        composite images (with known colony locations) &rarr; train a YOLOv8
        detector on this synthetic dataset. They created thousands of labeled
        synthetic images in this manner, essentially for free. The only manual
        step was sorting out a few SAM mistakes (e.g., SAM struggled with very </span
      ><span class="c1 c7">blurry colonies</span
      ><span class="c3 c1">, which required manual removal).</span>
    </p>
    <p class="c13"><span class="c3 c1"></span></p>
    <p class="c8">
      <span class="c0">Figure: </span
      ><span class="c0 c7">SAM-driven augmentation pipeline</span
      ><span class="c10 c0"
        >&nbsp;for bacterial colony detection. Real plate images (left) are
        processed by SAM to extract colony masks (top center). After filtering
        out poor masks, the high-quality cutouts are saved to a database. New
        synthetic images are generated by pasting these colonies into empty agar
        images (bottom center). A YOLOv8 detector is first trained on this
        synthetic data (bottom) and then fine-tuned on a small set of real
        images. The result (right) is an accurate colony detection model without
        needing a large real dataset.</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <p class="c8">
      <span class="c1"
        >The impact of this approach was striking. With only ~100 real images
        plus synthetic augmentation, the YOLOv8 model achieved almost the </span
      ><span class="c1 c7"
        >same accuracy as training on a 5&times; larger real dataset</span
      ><span class="c1"
        >. Specifically, 1000 real images + SAM-augmented synthetic data reached
        a mean average precision (mAP) just a few points shy of a model trained
        on 5241 real images. Even with as few as 50 real images, the synthetic
        data boosted detection performance well above models trained on 50 real
        images alone. This approach underscores how foundation models like SAM
        can </span
      ><span class="c0">compress the data requirement</span
      ><span class="c3 c1"
        >&nbsp;for new tasks by enabling good results with far less labeled data
        by generating additional training examples. For academia and startups
        alike, that means faster development of vision assays (less time
        photographing and hand-labeling thousands of examples, more time getting
        results).</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <p class="c8">
      <span class="c1">Interestingly, while this pipeline focused on </span
      ><span class="c1 c7">detecting all colonies</span
      ><span class="c1"
        >&nbsp;for hygiene monitoring (just finding any growth), the authors
        note that they didn&#39;t yet exploit </span
      ><span class="c1 c7">morphological features</span
      ><span class="c1">, like colony </span><span class="c0">shape</span
      ><span class="c1">&nbsp;or </span><span class="c0">color,</span
      ><span class="c1"
        >&nbsp;in their detection task. In applications like species
        identification or assessing colony health, those features matter since
        they suggest future work could incorporate analyses of colony </span
      ><span class="c1 c7">pigmentation or texture</span
      ><span class="c3 c1"
        >. That&#39;s a perfect segue into how DINO and Grounded DINO can help
        label or filter segments by such visual traits.</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <h2 class="c8" id="h.xovysovv97f9">
      <span class="c1 c9"
        >DINO: Self-Supervised Vision for Unlabeled Biological Phenomena</span
      >
    </h2>
    <p class="c8">
      <span class="c1"
        >While SAM excels at drawing masks, it doesn&#39;t tell you what those
        masks </span
      ><span class="c0">are</span
      ><span class="c1">&nbsp;or how they differ. </span
      ><span class="c1 c7">DINO</span
      ><span class="c1">&nbsp;(and the updated </span
      ><span class="c1 c7">DINOv2</span
      ><span class="c1"
        >) are vision transformer models trained in a self-supervised manner to
        learn rich image features. In practical terms, DINO learns to encode
        images (or image patches) into a feature space where similar-looking
        things cluster together (all without any human labels. In 2023, a team
        of researchers applied DINO to microscopy images and found it had a </span
      ><span class="c0">remarkable ability to learn cellular morphology</span
      ><span class="c1">&nbsp;without supervision. Doron </span
      ><span class="c0">et al.</span
      ><span class="c1"
        >&nbsp;showed that DINO&#39;s features of single-cell images were so
        meaningful that simple classifiers built on them could distinguish cell
        types and even subtle phenotypic differences nearly as well as highly
        engineered, task-specific features. In their words, </span
      ><span class="c0"
        >&quot;DINO, a vision-transformer based self-supervised algorithm, has a
        remarkable ability for learning rich representations of cellular
        morphology&quot;</span
      ><span class="c3 c1"
        >. These representations were biologically faithful: different DINO
        attention heads even aligned with different subcellular structures, like
        nucleus versus cytoplasm, across thousands of cells.</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <p class="c8">
      <span class="c1">For bioimage analysis, DINO opens the door to </span
      ><span class="c1 c7">unsupervised clustering and labeling</span
      ><span class="c1"
        >&nbsp;of image data. Imagine you have segmented hundreds of bacterial
        colonies with SAM. Some have </span
      ><span class="c0">red </span
      ><span class="c1">pigmentations, some others have a </span
      ><span class="c0">white </span
      ><span class="c1"
        >pigment; meanwhile, some have smooth edges, and others are more
        irregular. You might not have labels for these traits in advance. A
        model like DINO can embed each colony image into a vector, such that
        colonies with similar appearance group together in feature space.
        Indeed, researchers have used self-supervised features to cluster cell
        images by morphology or response to drugs, revealing meaningful
        groupings without explicit labels. We can leverage this by taking
        SAM&#39;s unlabeled masks and using DINO embeddings to </span
      ><span class="c1 c7">filter or organize them by visual traits</span
      ><span class="c3 c1"
        >. For example, one could automatically separate pigmented vs
        non-pigmented colonies by clustering the mask crops in DINO feature
        space, then label those clusters post hoc (or just automatically measure
        their color if it&#39;s as simple as hue). The key benefit is reducing
        manual labor: instead of inspecting each mask, the model&#39;s learned
        features do the heavy lifting.</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <p class="c8">
      <span class="c1"
        >In our projects, we found this helpful for things like </span
      ><span class="c1 c7">colony pigmentation detection</span
      ><span class="c1"
        >. We combined SAM with DINO to identify which segmented colonies on a
        plate were producing a specific colored pigment (a common screening
        method in synthetic biology). SAM provided all colony regions, and
        DINO&#39;s representation helped spot the oddballs (e.g., only a subset
        of colonies had a dark red hue from the standard dark purple and those
        clustered apart from the cream-colored ones in the feature space). This
        way, we could flag pigmented colonies automatically. Recent research
        supports this approach: self-supervised ViTs can capture even subtle
        differences like fluorescent reporter expression or morphological
        changes due to drugs. Essentially, foundation models can </span
      ><span class="c1 c7">segment and characterize</span
      ><span class="c1"
        >&nbsp;biological samples in a two-step, label-free workflow: SAM
        handles </span
      ><span class="c0">&quot;where&quot;, </span
      ><span class="c1">while DINO handles </span
      ><span class="c0">&quot;what&#39;s different&quot;</span
      ><span class="c3 c1">.</span>
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <h2 class="c8" id="h.ymplf5tl06ua">
      <span class="c9 c1">Grounding DINO: Segmenting by Text Prompts</span>
    </h2>
    <p class="c8">
      <span class="c1"
        >An even more direct way to filter or label segments by their traits is
        to use language. </span
      ><span class="c1 c7">Grounding DINO</span
      ><span class="c1"
        >&nbsp;is a vision-language model that extends DINO&#39;s detection
        abilities to work with text queries. It&#39;s a </span
      ><span class="c1 c7">zero-shot detector</span
      ><span class="c1"
        >&nbsp;that can draw bounding boxes around objects described by a
        prompt, like &quot;brown colony&quot; or &quot;clear zone&quot;, even if
        it&#39;s never seen those exact items before. Combining it with SAM
        (dubbed &quot;Grounded-SAM&quot;) allows an almost sci-fi workflow: </span
      ><span class="c1 c7"
        >type what you want to segment, and the models will find and mask
        it</span
      ><span class="c3 c1"
        >. For example, in &quot;natural&quot; or &quot;commonly human&quot;
        pictures, you could ask for &quot;cat&quot;, and Grounding DINO will
        localize the cat while SAM segments it precisely. In the bio lab
        setting, one might prompt &quot;white bacterial colony&quot; versus
        &quot;red bacterial colony&quot; to have the system pick out colonies of
        each color and mask them separately with no manual clicking
        required.</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <p class="c8">
      <span class="c1"
        >While this is cutting-edge, we are starting to see it in practice. A
        recent demo applied </span
      ><span class="c1 c7">Grounding DINO + SAM on agricultural images</span
      ><span class="c1"
        >&nbsp;to detect plant seedlings by just describing them, and it worked </span
      ><span class="c1 c7">without any fine-tuning on those images</span
      ><span class="c1"
        >. In our lab, we&#39;ve experimented with Grounded-SAM for tasks like
        identifying </span
      ><span class="c0">plate contaminants</span
      ><span class="c1"
        >. By providing text like &quot;fungal colony&quot; or &quot;bubble
        artifact&quot;, the model can attempt to highlight regions that match
        those descriptions, which SAM then segments. Of course, the accuracy
        depends on how well the text prompt aligns with the model&#39;s learned
        knowledge. Describing visual traits (color, shape) tends to be easier
        (e.g., &quot;circular clear areas&quot; might help find antibiotic
        inhibition zones); whereas very domain-specific terms might not work
        unless the model has seen similar data in training. Still, this approach
        of </span
      ><span class="c1 c7">open-vocabulary segmentation</span
      ><span class="c1"
        >&nbsp;is extremely powerful as a concept: it means we can issue
        high-level instructions to images and get structured outputs with a
        drastic annotation speed up. As Piotr Skalski noted, combining Grounding
        DINO&#39;s detection with SAM&#39;s mask generation can </span
      ><span class="c1 c7">turbocharge image annotation</span
      ><span class="c3 c1"
        >, saving huge amounts of time for building new datasets.</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <p class="c8">
      <span class="c1"
        >For filtering SAM&#39;s many masks, one can imagine Grounding DINO
        acting as a selector. Given SAM&#39;s pile of segments, we ask Grounding
        DINO (or a similar vision-language model) to &quot;find the brown colony
        among these&quot; and only keep masks overlapping with the detections.
        This is a form of multimodal filtering: using language as a criterion
        for vision, which is especially handy when the trait of interest is
        easier to describe than to quantify formally. We anticipate more tools
        and libraries building on Grounded-SAM pipelines since the
        &quot;Grounded Segment Anything&quot; GitHub project has already
        assembled demos that integrate Grounding DINO, SAM, and even Stable
        Diffusion for various &quot;detect and segment anything&quot; tasks. The
        tech is evolving rapidly, and savvy bioimage analysts can start
        harnessing it for tasks like colony phenotype sorting, locating specific
        structures in microscopy slides via text (e.g., &quot;mitotic
        figure&quot;, &quot;necrotic region&quot;), and so on (all </span
      ><span class="c1 c7">without training a new model</span
      ><span class="c1 c3">&nbsp;for each task).</span>
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <h2 class="c8" id="h.95t92p83p4m">
      <span class="c9 c1">Towards Automated Lab Workflows</span>
    </h2>
    <p class="c8">
      <span class="c1">The ultimate promise of these models is </span
      ><span class="c1 c7">AI-driven pipelines</span
      ><span class="c1"
        >&nbsp;that automatically handle routine image analysis in the lab.
        We&#39;re already seeing prototypes of this. In high-throughput
        screening, researchers can analyze images from multi-well plates or GPS
        satellites on the fly: for instance, a smart microscope might quickly
        run a YOLO detector to see if any wells have &quot;hits&quot;
        (interesting growth or fluorescence), and if so, use SAM to segment the
        region and measure it to automatedly decide how to proceed in real time.
        M&uuml;ller </span
      ><span class="c0">et al.</span
      ><span class="c1"
        >&nbsp;describe this scenario in exactly the following way: acquire a
        quick low-res image, use a model to decide if something worthy is
        present, and, if not, skip saving detailed data for that well. Such
        decision-making could make experiments more efficient (huge data savings
        when most wells are empty or negative). As they note, while YOLO gives a
        rough localization (bounding box), </span
      ><span class="c1 c7"
        >&quot;biological objects are typically not square&quot;</span
      ><span class="c1"
        >, so you can feed the detection into SAM to get an accurate shape mask
        for precise measurement. This kind of cascade (detection, then
        segmentation) is compelling and impactful. We used a similar idea for
        analyzing </span
      ><span class="c1 c7">phage plaque assays</span
      ><span class="c3 c1"
        >&nbsp;(where viruses create clear spots in a bacterial lawn). First, a
        detection model spots where there are clear zones on a plate, and then
        SAM (prompted with those locations) segments the exact cleared area so
        we can measure its size. In classical analysis of antibiotic or phage
        inhibition zones, one would do this by thresholding or edge detection on
        the clear region. However, SAM gives us an instant, accurate outline of
        even irregularly shaped zones; there is no need to hand-tune intensity
        thresholds since the model &quot;sees&quot; the absence of bacteria and
        delineates it.</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <p class="c8">
      <span class="c1">Incorporating these into </span
      ><span class="c1 c7">user-friendly tools</span
      ><span class="c1"
        >&nbsp;is a current challenge, but progress is steady. For example, </span
      ><span class="c1 c7">IAMSAM</span
      ><span class="c1"
        >, a web tool for spatial transcriptomics, uses SAM under the hood to
        let researchers segment tissue regions by morphology and then correlate
        those regions with gene expression data. It allows semi-automatic
        selection of regions of interest (say, clusters of cells in a tissue),
        which would have taken far longer to draw manually. On the industry
        side, even major microscope software are embracing foundation models:
        ZEISS&#39;s ZEN software recently added </span
      ><span class="c1 c7"
        >&quot;super-fast image annotations with SAM&quot;</span
      ><span class="c3 c1"
        >&nbsp;in their cloud platform. This means biologists can leverage
        SAM&#39;s mask generation on their images through a vendor interface,
        speeding up annotation for further analysis. Startups and biotech
        companies are also watching closely. The ability to process an entire
        multi-well plate of images in seconds, segment all colonies or cell
        clusters, and flag interesting ones (maybe with an AI assistant like
        &quot;Plato&quot; as one lab automation platform describes) is
        incredibly appealing for scaling up experiments. The technology is
        catching up to these needs: efficient versions of SAM (like MobileSAM
        and EfficientSAM) are emerging to run on everyday hardware, making it
        feasible to deploy these models in lab settings without requiring an AI
        specialist for each use.</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <h2 class="c8" id="h.c6uyiziw7zr3">
      <span class="c9 c1">Conclusion</span>
    </h2>
    <p class="c8">
      <span class="c1"
        >We stand at a crossroads where general AI vision models trained on
        internet-scale data meet the specialized world of biological imaging. </span
      ><span class="c1 c7">SAM</span
      ><span class="c1"
        >&nbsp;has shown that a single model can generalize to segment cells,
        colonies, organoids (essentially, </span
      ><span class="c0 c7">anything</span><span class="c1">)</span
      ><span class="c1"
        >&nbsp;with zero or minimal retraining, given the right prompts. </span
      ><span class="c1 c7">DINO</span
      ><span class="c1"
        >&nbsp;and its variants demonstrate that even without labels, AI can
        learn the subtle visual signatures of biological phenomena, from the
        morphology of a single cell to the pigmentation of a bacterial colony.
        With </span
      ><span class="c1 c7">Grounded DINO</span
      ><span class="c3 c1"
        >, we can even speak to our images in natural language, pulling out the
        information we care about (&quot;find the GFP-expressing cell
        clumps&quot; or &quot;count the clear plaques&quot;).</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <p class="c8">
      <span class="c3 c1"
        >The future of bioimage analysis will likely be a synergy of these
        foundation models with domain-specific knowledge. We&#39;ll see more
        pipelines where a general model handles the heavy lifting (segmentation,
        feature extraction, detection) and a lightweight custom layer handles
        the specifics (filtering by shape, linking to experimental metadata,
        etc.). For academics, this lowers the barrier to analyzing complex
        datasets since you can bootstrap analysis with SAM+DINO and focus your
        precious annotation time only on refining the outputs. For startups, it
        means faster development of imaging products (no need to collect a
        million examples of every new assay; a foundation model plus a few-shot
        fine-tune might suffice). For recruiters in biotech/AI, it&#39;s clear
        that familiarity with these tools is becoming a sought-after skill: they
        enable smaller teams to achieve what only big companies with massive
        training datasets could do a few years ago.</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <p class="c8">
      <span class="c1"
        >In closing, &quot;segmenting the invisible&quot; is no longer science
        fiction in biology. We can now literally segment things we couldn&#39;t
        even properly label before. A Petri dish teeming with microcolonies or a
        multiplexed cell image with dozens of phenotypes, any of these can be
        navigated and quantified with the help of SAM, DINO, and their cousins.
        As we continue </span
      ><span class="c1 c7">segmenting the invisible</span
      ><span class="c1">, we make the once-inscrutable data far more </span
      ><span class="c1 c7">visible and actionable</span
      ><span class="c3 c1"
        >. The hope is that this leads to quicker discoveries (finding that one
        odd colony indicates a breakthrough mutant), more efficient workflows
        (automating tedious image scoring), and, ultimately, a deeper
        understanding of biological systems through the lens of cutting-edge AI.
        The invisible world is becoming a bit more visible, one segment at a
        time.</span
      >
    </p>
    <p class="c4"><span class="c2 c1"></span></p>
    <h2 class="c8" id="h.v8gjmp1utni2">
      <span class="c9 c1">References and Further Reading</span>
    </h2>
    <ul class="c16 lst-kix_pq6l9j27g4h8-0 start">
      <li class="c6 li-bullet-0">
        <span class="c1">Kirillov et al. </span
        ><span class="c0">&quot;Segment Anything.&quot;</span
        ><span class="c3 c1">&nbsp;ICCV 2023 &ndash; Introduction of SAM.</span>
      </li>
      <li class="c6 li-bullet-0">
        <span class="c1">Archit et al. </span
        ><span class="c0"
          >&quot;Segment Anything for Microscopy (&mu;SAM).&quot;</span
        ><span class="c3 c1"
          >&nbsp;Nature Methods 22, 579&ndash;591 (2025) &ndash; Fine-tuning SAM
          for bioimages.</span
        >
      </li>
      <li class="c6 li-bullet-0">
        <span class="c1">Ilic et al. </span
        ><span class="c0"
          >&quot;Analysis of Microbiological Samples using SAM.&quot;</span
        ><span class="c3 c1"
          >&nbsp;(Conf. paper, 2023) &ndash; Applying SAM to segment colonies in
          AGAR dataset.</span
        >
      </li>
      <li class="c6 li-bullet-0">
        <span class="c1">Kehl et al. </span
        ><span class="c0"
          >&quot;SAM-based Synthetic Data Augmentation for Colony
          Detection.&quot;</span
        ><span class="c3 c1"
          >&nbsp;Appl. Sci. 15(3):1260 (2023) &ndash; Pipeline using SAM to
          generate synthetic training data for colony counting.</span
        >
      </li>
      <li class="c6 li-bullet-0">
        <span class="c1">Doron et al. </span
        ><span class="c0"
          >&quot;Unbiased single-cell morphology with self-supervised vision
          transformers.&quot;</span
        ><span class="c3 c1"
          >&nbsp;bioRxiv 2023 &ndash; DINO captures rich cell morphology
          features without labels.</span
        >
      </li>
      <li class="c6 li-bullet-0">
        <span class="c1"
          >Grounding DINO GitHub &ndash; Open-vocabulary object detection model
          (2023). See also the </span
        ><span class="c0">Grounded-SAM</span
        ><span class="c3 c1"
          >&nbsp;project combining text prompts, detection, and SAM
          segmentation.</span
        >
      </li>
      <li class="c6 li-bullet-0">
        <span class="c1">Skalski, </span
        ><span class="c0"
          >&quot;Zero-Shot Image Annotation with Grounding DINO and
          SAM.&quot;</span
        ><span class="c3 c1"
          >&nbsp;Roboflow Blog, Apr 2023 &ndash; Tutorial on using Grounded DINO
          + SAM for faster dataset labeling.</span
        >
      </li>
      <li class="c6 li-bullet-0">
        <span class="c1">Pape et al. </span
        ><span class="c0"
          >&quot;IAMSAM: Image-based analysis of molecular signatures using
          SAM.&quot;</span
        ><span class="c3 c1"
          >&nbsp;Genome Biology 25:290 (2024) &ndash; Tool for spatial
          transcriptomics using SAM to segment tissue regions.</span
        >
      </li>
      <li class="c6 li-bullet-0">
        <span class="c1">Zoccoler et al. </span
        ><span class="c0">BiAPoL Blog</span
        ><span class="c3 c1"
          >, Feb 2024 &ndash; Tips on combining YOLO detectors with SAM for
          better segmentation in microscopy; introduces micro-SAM.</span
        >
      </li>
      <li class="c6 li-bullet-0">
        <span class="c3 c1"
          >Zeiss ZEN 3.11 Release Notes (2023) &ndash; Mentions integration of
          SAM for image annotation in microscopy software.</span
        >
      </li>
    </ul>
    <p class="c18"><span class="c3 c14"></span></p>
  </body>
</html>
