import{j as n}from"./chunk-BXEcVBbX.js";import{r as e,R as t,f as a}from"./chunk-DRBa9aRh.js";import{H as i}from"./index-v_LsQPP8.js";import"./chunk-C_tfXL3w.js";import"./transformers.web.js-BNI60vAQ.js";
/*! @license DOMPurify 3.2.6 | (c) Cure53 and other contributors | Released under the Apache license 2.0 and Mozilla Public License 2.0 | github.com/cure53/DOMPurify/blob/3.2.6/LICENSE */const{entries:s,setPrototypeOf:o,isFrozen:l,getPrototypeOf:r,getOwnPropertyDescriptor:c}=Object;let{freeze:p,seal:d,create:h}=Object,{apply:u,construct:g}="undefined"!=typeof Reflect&&Reflect;p||(p=function(n){return n}),d||(d=function(n){return n}),u||(u=function(n,e,t){return n.apply(e,t)}),g||(g=function(n,e){return new n(...e)});const m=D(Array.prototype.forEach),f=D(Array.prototype.lastIndexOf),y=D(Array.prototype.pop),b=D(Array.prototype.push),w=D(Array.prototype.splice),k=D(String.prototype.toLowerCase),v=D(String.prototype.toString),x=D(String.prototype.match),_=D(String.prototype.replace),j=D(String.prototype.indexOf),S=D(String.prototype.trim),A=D(Object.prototype.hasOwnProperty),I=D(RegExp.prototype.test),q=(T=TypeError,function(){for(var n=arguments.length,e=new Array(n),t=0;t<n;t++)e[t]=arguments[t];return g(T,e)});var T;function D(n){return function(e){e instanceof RegExp&&(e.lastIndex=0);for(var t=arguments.length,a=new Array(t>1?t-1:0),i=1;i<t;i++)a[i-1]=arguments[i];return u(n,e,a)}}function z(n,e){let t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:k;o&&o(n,null);let a=e.length;for(;a--;){let i=e[a];if("string"==typeof i){const n=t(i);n!==i&&(l(e)||(e[a]=n),i=n)}n[i]=!0}return n}function N(n){for(let e=0;e<n.length;e++){A(n,e)||(n[e]=null)}return n}function M(n){const e=h(null);for(const[t,a]of s(n)){A(n,t)&&(Array.isArray(a)?e[t]=N(a):a&&"object"==typeof a&&a.constructor===Object?e[t]=M(a):e[t]=a)}return e}function E(n,e){for(;null!==n;){const t=c(n,e);if(t){if(t.get)return D(t.get);if("function"==typeof t.value)return D(t.value)}n=r(n)}return function(){return null}}const O=p(["a","abbr","acronym","address","area","article","aside","audio","b","bdi","bdo","big","blink","blockquote","body","br","button","canvas","caption","center","cite","code","col","colgroup","content","data","datalist","dd","decorator","del","details","dfn","dialog","dir","div","dl","dt","element","em","fieldset","figcaption","figure","font","footer","form","h1","h2","h3","h4","h5","h6","head","header","hgroup","hr","html","i","img","input","ins","kbd","label","legend","li","main","map","mark","marquee","menu","menuitem","meter","nav","nobr","ol","optgroup","option","output","p","picture","pre","progress","q","rp","rt","ruby","s","samp","section","select","shadow","small","source","spacer","span","strike","strong","style","sub","summary","sup","table","tbody","td","template","textarea","tfoot","th","thead","time","tr","track","tt","u","ul","var","video","wbr"]),C=p(["svg","a","altglyph","altglyphdef","altglyphitem","animatecolor","animatemotion","animatetransform","circle","clippath","defs","desc","ellipse","filter","font","g","glyph","glyphref","hkern","image","line","lineargradient","marker","mask","metadata","mpath","path","pattern","polygon","polyline","radialgradient","rect","stop","style","switch","symbol","text","textpath","title","tref","tspan","view","vkern"]),R=p(["feBlend","feColorMatrix","feComponentTransfer","feComposite","feConvolveMatrix","feDiffuseLighting","feDisplacementMap","feDistantLight","feDropShadow","feFlood","feFuncA","feFuncB","feFuncG","feFuncR","feGaussianBlur","feImage","feMerge","feMergeNode","feMorphology","feOffset","fePointLight","feSpecularLighting","feSpotLight","feTile","feTurbulence"]),L=p(["animate","color-profile","cursor","discard","font-face","font-face-format","font-face-name","font-face-src","font-face-uri","foreignobject","hatch","hatchpath","mesh","meshgradient","meshpatch","meshrow","missing-glyph","script","set","solidcolor","unknown","use"]),F=p(["math","menclose","merror","mfenced","mfrac","mglyph","mi","mlabeledtr","mmultiscripts","mn","mo","mover","mpadded","mphantom","mroot","mrow","ms","mspace","msqrt","mstyle","msub","msup","msubsup","mtable","mtd","mtext","mtr","munder","munderover","mprescripts"]),P=p(["maction","maligngroup","malignmark","mlongdiv","mscarries","mscarry","msgroup","mstack","msline","msrow","semantics","annotation","annotation-xml","mprescripts","none"]),W=p(["#text"]),U=p(["accept","action","align","alt","autocapitalize","autocomplete","autopictureinpicture","autoplay","background","bgcolor","border","capture","cellpadding","cellspacing","checked","cite","class","clear","color","cols","colspan","controls","controlslist","coords","crossorigin","datetime","decoding","default","dir","disabled","disablepictureinpicture","disableremoteplayback","download","draggable","enctype","enterkeyhint","face","for","headers","height","hidden","high","href","hreflang","id","inputmode","integrity","ismap","kind","label","lang","list","loading","loop","low","max","maxlength","media","method","min","minlength","multiple","muted","name","nonce","noshade","novalidate","nowrap","open","optimum","pattern","placeholder","playsinline","popover","popovertarget","popovertargetaction","poster","preload","pubdate","radiogroup","readonly","rel","required","rev","reversed","role","rows","rowspan","spellcheck","scope","selected","shape","size","sizes","span","srclang","start","src","srcset","step","style","summary","tabindex","title","translate","type","usemap","valign","value","width","wrap","xmlns","slot"]),B=p(["accent-height","accumulate","additive","alignment-baseline","amplitude","ascent","attributename","attributetype","azimuth","basefrequency","baseline-shift","begin","bias","by","class","clip","clippathunits","clip-path","clip-rule","color","color-interpolation","color-interpolation-filters","color-profile","color-rendering","cx","cy","d","dx","dy","diffuseconstant","direction","display","divisor","dur","edgemode","elevation","end","exponent","fill","fill-opacity","fill-rule","filter","filterunits","flood-color","flood-opacity","font-family","font-size","font-size-adjust","font-stretch","font-style","font-variant","font-weight","fx","fy","g1","g2","glyph-name","glyphref","gradientunits","gradienttransform","height","href","id","image-rendering","in","in2","intercept","k","k1","k2","k3","k4","kerning","keypoints","keysplines","keytimes","lang","lengthadjust","letter-spacing","kernelmatrix","kernelunitlength","lighting-color","local","marker-end","marker-mid","marker-start","markerheight","markerunits","markerwidth","maskcontentunits","maskunits","max","mask","media","method","mode","min","name","numoctaves","offset","operator","opacity","order","orient","orientation","origin","overflow","paint-order","path","pathlength","patterncontentunits","patterntransform","patternunits","points","preservealpha","preserveaspectratio","primitiveunits","r","rx","ry","radius","refx","refy","repeatcount","repeatdur","restart","result","rotate","scale","seed","shape-rendering","slope","specularconstant","specularexponent","spreadmethod","startoffset","stddeviation","stitchtiles","stop-color","stop-opacity","stroke-dasharray","stroke-dashoffset","stroke-linecap","stroke-linejoin","stroke-miterlimit","stroke-opacity","stroke","stroke-width","style","surfacescale","systemlanguage","tabindex","tablevalues","targetx","targety","transform","transform-origin","text-anchor","text-decoration","text-rendering","textlength","type","u1","u2","unicode","values","viewbox","visibility","version","vert-adv-y","vert-origin-x","vert-origin-y","width","word-spacing","wrap","writing-mode","xchannelselector","ychannelselector","x","x1","x2","xmlns","y","y1","y2","z","zoomandpan"]),G=p(["accent","accentunder","align","bevelled","close","columnsalign","columnlines","columnspan","denomalign","depth","dir","display","displaystyle","encoding","fence","frame","height","href","id","largeop","length","linethickness","lspace","lquote","mathbackground","mathcolor","mathsize","mathvariant","maxsize","minsize","movablelimits","notation","numalign","open","rowalign","rowlines","rowspacing","rowspan","rspace","rquote","scriptlevel","scriptminsize","scriptsizemultiplier","selection","separator","separators","stretchy","subscriptshift","supscriptshift","symmetric","voffset","width","xmlns"]),H=p(["xlink:href","xml:id","xlink:title","xml:space","xmlns:xlink"]),Y=d(/\{\{[\w\W]*|[\w\W]*\}\}/gm),K=d(/<%[\w\W]*|[\w\W]*%>/gm),V=d(/\$\{[\w\W]*/gm),Z=d(/^data-[\-\w.\u00B7-\uFFFF]+$/),Q=d(/^aria-[\-\w]+$/),X=d(/^(?:(?:(?:f|ht)tps?|mailto|tel|callto|sms|cid|xmpp|matrix):|[^a-z]|[a-z+.\-]+(?:[^a-z+.\-:]|$))/i),J=d(/^(?:\w+script|data):/i),$=d(/[\u0000-\u0020\u00A0\u1680\u180E\u2000-\u2029\u205F\u3000]/g),nn=d(/^html$/i),en=d(/^[a-z][.\w]*(-[.\w]+)+$/i);var tn=Object.freeze({__proto__:null,ARIA_ATTR:Q,ATTR_WHITESPACE:$,CUSTOM_ELEMENT:en,DATA_ATTR:Z,DOCTYPE_NAME:nn,ERB_EXPR:K,IS_ALLOWED_URI:X,IS_SCRIPT_OR_DATA:J,MUSTACHE_EXPR:Y,TMPLIT_EXPR:V});const an=1,sn=3,on=7,ln=8,rn=9;var cn=function n(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"undefined"==typeof window?null:window;const t=e=>n(e);if(t.version="3.2.6",t.removed=[],!e||!e.document||e.document.nodeType!==rn||!e.Element)return t.isSupported=!1,t;let{document:a}=e;const i=a,o=i.currentScript,{DocumentFragment:l,HTMLTemplateElement:r,Node:c,Element:d,NodeFilter:u,NamedNodeMap:g=e.NamedNodeMap||e.MozNamedAttrMap,HTMLFormElement:T,DOMParser:D,trustedTypes:N}=e,Y=d.prototype,K=E(Y,"cloneNode"),V=E(Y,"remove"),Z=E(Y,"nextSibling"),Q=E(Y,"childNodes"),J=E(Y,"parentNode");if("function"==typeof r){const n=a.createElement("template");n.content&&n.content.ownerDocument&&(a=n.content.ownerDocument)}let $,en="";const{implementation:cn,createNodeIterator:pn,createDocumentFragment:dn,getElementsByTagName:hn}=a,{importNode:un}=i;let gn={afterSanitizeAttributes:[],afterSanitizeElements:[],afterSanitizeShadowDOM:[],beforeSanitizeAttributes:[],beforeSanitizeElements:[],beforeSanitizeShadowDOM:[],uponSanitizeAttribute:[],uponSanitizeElement:[],uponSanitizeShadowNode:[]};t.isSupported="function"==typeof s&&"function"==typeof J&&cn&&void 0!==cn.createHTMLDocument;const{MUSTACHE_EXPR:mn,ERB_EXPR:fn,TMPLIT_EXPR:yn,DATA_ATTR:bn,ARIA_ATTR:wn,IS_SCRIPT_OR_DATA:kn,ATTR_WHITESPACE:vn,CUSTOM_ELEMENT:xn}=tn;let{IS_ALLOWED_URI:_n}=tn,jn=null;const Sn=z({},[...O,...C,...R,...F,...W]);let An=null;const In=z({},[...U,...B,...G,...H]);let qn=Object.seal(h(null,{tagNameCheck:{writable:!0,configurable:!1,enumerable:!0,value:null},attributeNameCheck:{writable:!0,configurable:!1,enumerable:!0,value:null},allowCustomizedBuiltInElements:{writable:!0,configurable:!1,enumerable:!0,value:!1}})),Tn=null,Dn=null,zn=!0,Nn=!0,Mn=!1,En=!0,On=!1,Cn=!0,Rn=!1,Ln=!1,Fn=!1,Pn=!1,Wn=!1,Un=!1,Bn=!0,Gn=!1,Hn=!0,Yn=!1,Kn={},Vn=null;const Zn=z({},["annotation-xml","audio","colgroup","desc","foreignobject","head","iframe","math","mi","mn","mo","ms","mtext","noembed","noframes","noscript","plaintext","script","style","svg","template","thead","title","video","xmp"]);let Qn=null;const Xn=z({},["audio","video","img","source","image","track"]);let Jn=null;const $n=z({},["alt","class","for","id","label","name","pattern","placeholder","role","summary","title","value","style","xmlns"]),ne="http://www.w3.org/1998/Math/MathML",ee="http://www.w3.org/2000/svg",te="http://www.w3.org/1999/xhtml";let ae=te,ie=!1,se=null;const oe=z({},[ne,ee,te],v);let le=z({},["mi","mo","mn","ms","mtext"]),re=z({},["annotation-xml"]);const ce=z({},["title","style","font","a","script"]);let pe=null;const de=["application/xhtml+xml","text/html"];let he=null,ue=null;const ge=a.createElement("form"),me=function(n){return n instanceof RegExp||n instanceof Function},fe=function(){let n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};if(!ue||ue!==n){if(n&&"object"==typeof n||(n={}),n=M(n),pe=-1===de.indexOf(n.PARSER_MEDIA_TYPE)?"text/html":n.PARSER_MEDIA_TYPE,he="application/xhtml+xml"===pe?v:k,jn=A(n,"ALLOWED_TAGS")?z({},n.ALLOWED_TAGS,he):Sn,An=A(n,"ALLOWED_ATTR")?z({},n.ALLOWED_ATTR,he):In,se=A(n,"ALLOWED_NAMESPACES")?z({},n.ALLOWED_NAMESPACES,v):oe,Jn=A(n,"ADD_URI_SAFE_ATTR")?z(M($n),n.ADD_URI_SAFE_ATTR,he):$n,Qn=A(n,"ADD_DATA_URI_TAGS")?z(M(Xn),n.ADD_DATA_URI_TAGS,he):Xn,Vn=A(n,"FORBID_CONTENTS")?z({},n.FORBID_CONTENTS,he):Zn,Tn=A(n,"FORBID_TAGS")?z({},n.FORBID_TAGS,he):M({}),Dn=A(n,"FORBID_ATTR")?z({},n.FORBID_ATTR,he):M({}),Kn=!!A(n,"USE_PROFILES")&&n.USE_PROFILES,zn=!1!==n.ALLOW_ARIA_ATTR,Nn=!1!==n.ALLOW_DATA_ATTR,Mn=n.ALLOW_UNKNOWN_PROTOCOLS||!1,En=!1!==n.ALLOW_SELF_CLOSE_IN_ATTR,On=n.SAFE_FOR_TEMPLATES||!1,Cn=!1!==n.SAFE_FOR_XML,Rn=n.WHOLE_DOCUMENT||!1,Pn=n.RETURN_DOM||!1,Wn=n.RETURN_DOM_FRAGMENT||!1,Un=n.RETURN_TRUSTED_TYPE||!1,Fn=n.FORCE_BODY||!1,Bn=!1!==n.SANITIZE_DOM,Gn=n.SANITIZE_NAMED_PROPS||!1,Hn=!1!==n.KEEP_CONTENT,Yn=n.IN_PLACE||!1,_n=n.ALLOWED_URI_REGEXP||X,ae=n.NAMESPACE||te,le=n.MATHML_TEXT_INTEGRATION_POINTS||le,re=n.HTML_INTEGRATION_POINTS||re,qn=n.CUSTOM_ELEMENT_HANDLING||{},n.CUSTOM_ELEMENT_HANDLING&&me(n.CUSTOM_ELEMENT_HANDLING.tagNameCheck)&&(qn.tagNameCheck=n.CUSTOM_ELEMENT_HANDLING.tagNameCheck),n.CUSTOM_ELEMENT_HANDLING&&me(n.CUSTOM_ELEMENT_HANDLING.attributeNameCheck)&&(qn.attributeNameCheck=n.CUSTOM_ELEMENT_HANDLING.attributeNameCheck),n.CUSTOM_ELEMENT_HANDLING&&"boolean"==typeof n.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements&&(qn.allowCustomizedBuiltInElements=n.CUSTOM_ELEMENT_HANDLING.allowCustomizedBuiltInElements),On&&(Nn=!1),Wn&&(Pn=!0),Kn&&(jn=z({},W),An=[],!0===Kn.html&&(z(jn,O),z(An,U)),!0===Kn.svg&&(z(jn,C),z(An,B),z(An,H)),!0===Kn.svgFilters&&(z(jn,R),z(An,B),z(An,H)),!0===Kn.mathMl&&(z(jn,F),z(An,G),z(An,H))),n.ADD_TAGS&&(jn===Sn&&(jn=M(jn)),z(jn,n.ADD_TAGS,he)),n.ADD_ATTR&&(An===In&&(An=M(An)),z(An,n.ADD_ATTR,he)),n.ADD_URI_SAFE_ATTR&&z(Jn,n.ADD_URI_SAFE_ATTR,he),n.FORBID_CONTENTS&&(Vn===Zn&&(Vn=M(Vn)),z(Vn,n.FORBID_CONTENTS,he)),Hn&&(jn["#text"]=!0),Rn&&z(jn,["html","head","body"]),jn.table&&(z(jn,["tbody"]),delete Tn.tbody),n.TRUSTED_TYPES_POLICY){if("function"!=typeof n.TRUSTED_TYPES_POLICY.createHTML)throw q('TRUSTED_TYPES_POLICY configuration option must provide a "createHTML" hook.');if("function"!=typeof n.TRUSTED_TYPES_POLICY.createScriptURL)throw q('TRUSTED_TYPES_POLICY configuration option must provide a "createScriptURL" hook.');$=n.TRUSTED_TYPES_POLICY,en=$.createHTML("")}else void 0===$&&($=function(n,e){if("object"!=typeof n||"function"!=typeof n.createPolicy)return null;let t=null;const a="data-tt-policy-suffix";e&&e.hasAttribute(a)&&(t=e.getAttribute(a));const i="dompurify"+(t?"#"+t:"");try{return n.createPolicy(i,{createHTML:n=>n,createScriptURL:n=>n})}catch(s){return null}}(N,o)),null!==$&&"string"==typeof en&&(en=$.createHTML(""));p&&p(n),ue=n}},ye=z({},[...C,...R,...L]),be=z({},[...F,...P]),we=function(n){b(t.removed,{element:n});try{J(n).removeChild(n)}catch(e){V(n)}},ke=function(n,e){try{b(t.removed,{attribute:e.getAttributeNode(n),from:e})}catch(a){b(t.removed,{attribute:null,from:e})}if(e.removeAttribute(n),"is"===n)if(Pn||Wn)try{we(e)}catch(a){}else try{e.setAttribute(n,"")}catch(a){}},ve=function(n){let e=null,t=null;if(Fn)n="<remove></remove>"+n;else{const e=x(n,/^[\r\n\t ]+/);t=e&&e[0]}"application/xhtml+xml"===pe&&ae===te&&(n='<html xmlns="http://www.w3.org/1999/xhtml"><head></head><body>'+n+"</body></html>");const i=$?$.createHTML(n):n;if(ae===te)try{e=(new D).parseFromString(i,pe)}catch(o){}if(!e||!e.documentElement){e=cn.createDocument(ae,"template",null);try{e.documentElement.innerHTML=ie?en:i}catch(o){}}const s=e.body||e.documentElement;return n&&t&&s.insertBefore(a.createTextNode(t),s.childNodes[0]||null),ae===te?hn.call(e,Rn?"html":"body")[0]:Rn?e.documentElement:s},xe=function(n){return pn.call(n.ownerDocument||n,n,u.SHOW_ELEMENT|u.SHOW_COMMENT|u.SHOW_TEXT|u.SHOW_PROCESSING_INSTRUCTION|u.SHOW_CDATA_SECTION,null)},_e=function(n){return n instanceof T&&("string"!=typeof n.nodeName||"string"!=typeof n.textContent||"function"!=typeof n.removeChild||!(n.attributes instanceof g)||"function"!=typeof n.removeAttribute||"function"!=typeof n.setAttribute||"string"!=typeof n.namespaceURI||"function"!=typeof n.insertBefore||"function"!=typeof n.hasChildNodes)},je=function(n){return"function"==typeof c&&n instanceof c};function Se(n,e,a){m(n,n=>{n.call(t,e,a,ue)})}const Ae=function(n){let e=null;if(Se(gn.beforeSanitizeElements,n,null),_e(n))return we(n),!0;const a=he(n.nodeName);if(Se(gn.uponSanitizeElement,n,{tagName:a,allowedTags:jn}),Cn&&n.hasChildNodes()&&!je(n.firstElementChild)&&I(/<[/\w!]/g,n.innerHTML)&&I(/<[/\w!]/g,n.textContent))return we(n),!0;if(n.nodeType===on)return we(n),!0;if(Cn&&n.nodeType===ln&&I(/<[/\w]/g,n.data))return we(n),!0;if(!jn[a]||Tn[a]){if(!Tn[a]&&qe(a)){if(qn.tagNameCheck instanceof RegExp&&I(qn.tagNameCheck,a))return!1;if(qn.tagNameCheck instanceof Function&&qn.tagNameCheck(a))return!1}if(Hn&&!Vn[a]){const e=J(n)||n.parentNode,t=Q(n)||n.childNodes;if(t&&e){for(let a=t.length-1;a>=0;--a){const i=K(t[a],!0);i.__removalCount=(n.__removalCount||0)+1,e.insertBefore(i,Z(n))}}}return we(n),!0}return n instanceof d&&!function(n){let e=J(n);e&&e.tagName||(e={namespaceURI:ae,tagName:"template"});const t=k(n.tagName),a=k(e.tagName);return!!se[n.namespaceURI]&&(n.namespaceURI===ee?e.namespaceURI===te?"svg"===t:e.namespaceURI===ne?"svg"===t&&("annotation-xml"===a||le[a]):Boolean(ye[t]):n.namespaceURI===ne?e.namespaceURI===te?"math"===t:e.namespaceURI===ee?"math"===t&&re[a]:Boolean(be[t]):n.namespaceURI===te?!(e.namespaceURI===ee&&!re[a])&&!(e.namespaceURI===ne&&!le[a])&&!be[t]&&(ce[t]||!ye[t]):!("application/xhtml+xml"!==pe||!se[n.namespaceURI]))}(n)?(we(n),!0):"noscript"!==a&&"noembed"!==a&&"noframes"!==a||!I(/<\/no(script|embed|frames)/i,n.innerHTML)?(On&&n.nodeType===sn&&(e=n.textContent,m([mn,fn,yn],n=>{e=_(e,n," ")}),n.textContent!==e&&(b(t.removed,{element:n.cloneNode()}),n.textContent=e)),Se(gn.afterSanitizeElements,n,null),!1):(we(n),!0)},Ie=function(n,e,t){if(Bn&&("id"===e||"name"===e)&&(t in a||t in ge))return!1;if(Nn&&!Dn[e]&&I(bn,e));else if(zn&&I(wn,e));else if(!An[e]||Dn[e]){if(!(qe(n)&&(qn.tagNameCheck instanceof RegExp&&I(qn.tagNameCheck,n)||qn.tagNameCheck instanceof Function&&qn.tagNameCheck(n))&&(qn.attributeNameCheck instanceof RegExp&&I(qn.attributeNameCheck,e)||qn.attributeNameCheck instanceof Function&&qn.attributeNameCheck(e))||"is"===e&&qn.allowCustomizedBuiltInElements&&(qn.tagNameCheck instanceof RegExp&&I(qn.tagNameCheck,t)||qn.tagNameCheck instanceof Function&&qn.tagNameCheck(t))))return!1}else if(Jn[e]);else if(I(_n,_(t,vn,"")));else if("src"!==e&&"xlink:href"!==e&&"href"!==e||"script"===n||0!==j(t,"data:")||!Qn[n]){if(Mn&&!I(kn,_(t,vn,"")));else if(t)return!1}else;return!0},qe=function(n){return"annotation-xml"!==n&&x(n,xn)},Te=function(n){Se(gn.beforeSanitizeAttributes,n,null);const{attributes:e}=n;if(!e||_e(n))return;const a={attrName:"",attrValue:"",keepAttr:!0,allowedAttributes:An,forceKeepAttr:void 0};let i=e.length;for(;i--;){const o=e[i],{name:l,namespaceURI:r,value:c}=o,p=he(l),d=c;let h="value"===l?d:S(d);if(a.attrName=p,a.attrValue=h,a.keepAttr=!0,a.forceKeepAttr=void 0,Se(gn.uponSanitizeAttribute,n,a),h=a.attrValue,!Gn||"id"!==p&&"name"!==p||(ke(l,n),h="user-content-"+h),Cn&&I(/((--!?|])>)|<\/(style|title)/i,h)){ke(l,n);continue}if(a.forceKeepAttr)continue;if(!a.keepAttr){ke(l,n);continue}if(!En&&I(/\/>/i,h)){ke(l,n);continue}On&&m([mn,fn,yn],n=>{h=_(h,n," ")});const u=he(n.nodeName);if(Ie(u,p,h)){if($&&"object"==typeof N&&"function"==typeof N.getAttributeType)if(r);else switch(N.getAttributeType(u,p)){case"TrustedHTML":h=$.createHTML(h);break;case"TrustedScriptURL":h=$.createScriptURL(h)}if(h!==d)try{r?n.setAttributeNS(r,l,h):n.setAttribute(l,h),_e(n)?we(n):y(t.removed)}catch(s){ke(l,n)}}else ke(l,n)}Se(gn.afterSanitizeAttributes,n,null)},De=function n(e){let t=null;const a=xe(e);for(Se(gn.beforeSanitizeShadowDOM,e,null);t=a.nextNode();)Se(gn.uponSanitizeShadowNode,t,null),Ae(t),Te(t),t.content instanceof l&&n(t.content);Se(gn.afterSanitizeShadowDOM,e,null)};return t.sanitize=function(n){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},a=null,s=null,o=null,r=null;if(ie=!n,ie&&(n="\x3c!--\x3e"),"string"!=typeof n&&!je(n)){if("function"!=typeof n.toString)throw q("toString is not a function");if("string"!=typeof(n=n.toString()))throw q("dirty is not a string, aborting")}if(!t.isSupported)return n;if(Ln||fe(e),t.removed=[],"string"==typeof n&&(Yn=!1),Yn){if(n.nodeName){const e=he(n.nodeName);if(!jn[e]||Tn[e])throw q("root node is forbidden and cannot be sanitized in-place")}}else if(n instanceof c)a=ve("\x3c!----\x3e"),s=a.ownerDocument.importNode(n,!0),s.nodeType===an&&"BODY"===s.nodeName||"HTML"===s.nodeName?a=s:a.appendChild(s);else{if(!Pn&&!On&&!Rn&&-1===n.indexOf("<"))return $&&Un?$.createHTML(n):n;if(a=ve(n),!a)return Pn?null:Un?en:""}a&&Fn&&we(a.firstChild);const p=xe(Yn?n:a);for(;o=p.nextNode();)Ae(o),Te(o),o.content instanceof l&&De(o.content);if(Yn)return n;if(Pn){if(Wn)for(r=dn.call(a.ownerDocument);a.firstChild;)r.appendChild(a.firstChild);else r=a;return(An.shadowroot||An.shadowrootmode)&&(r=un.call(i,r,!0)),r}let d=Rn?a.outerHTML:a.innerHTML;return Rn&&jn["!doctype"]&&a.ownerDocument&&a.ownerDocument.doctype&&a.ownerDocument.doctype.name&&I(nn,a.ownerDocument.doctype.name)&&(d="<!DOCTYPE "+a.ownerDocument.doctype.name+">\n"+d),On&&m([mn,fn,yn],n=>{d=_(d,n," ")}),$&&Un?$.createHTML(d):d},t.setConfig=function(){fe(arguments.length>0&&void 0!==arguments[0]?arguments[0]:{}),Ln=!0},t.clearConfig=function(){ue=null,Ln=!1},t.isValidAttribute=function(n,e,t){ue||fe({});const a=he(n),i=he(e);return Ie(a,i,t)},t.addHook=function(n,e){"function"==typeof e&&b(gn[n],e)},t.removeHook=function(n,e){if(void 0!==e){const t=f(gn[n],e);return-1===t?void 0:w(gn[n],t,1)[0]}return y(gn[n])},t.removeHooks=function(n){gn[n]=[]},t.removeAllHooks=function(){gn={afterSanitizeAttributes:[],afterSanitizeElements:[],afterSanitizeShadowDOM:[],beforeSanitizeAttributes:[],beforeSanitizeElements:[],beforeSanitizeShadowDOM:[],uponSanitizeAttribute:[],uponSanitizeElement:[],uponSanitizeShadowNode:[]}},t}();const pn=({html:a,title:s,description:o,canonical:l})=>{!function({title:n,description:t,canonical:a}){e.useEffect(()=>{if(n&&(document.title=n.length>60?`${n.slice(0,57)}...`:n),t){const n="description";let e=document.querySelector(`meta[name="${n}"]`);e||(e=document.createElement("meta"),e.setAttribute("name",n),document.head.appendChild(e));const a=t.length>160?`${t.slice(0,157)}...`:t;e.setAttribute("content",a)}if(a){let n=document.querySelector('link[rel="canonical"]');n||(n=document.createElement("link"),n.setAttribute("rel","canonical"),document.head.appendChild(n)),n.setAttribute("href",a)}},[n,t,a])}({title:s,description:o,canonical:l});const r=t.useMemo(()=>cn.sanitize(a,{USE_PROFILES:{html:!0}}),[a]);return n.jsxs("div",{className:"min-h-screen bg-background",children:[n.jsx(i,{}),n.jsx("main",{className:"pt-16",children:n.jsx("section",{className:"py-12",children:n.jsx("div",{className:"container mx-auto px-4",children:n.jsx("article",{className:"max-w-3xl mx-auto prose prose-neutral dark:prose-invert",children:n.jsx("div",{dangerouslySetInnerHTML:{__html:r}})})})})})]})},dn={"automating-healthcare-diagnostics":{title:"Automating Healthcare Diagnostics | cargonriv.com Blog",description:"Lessons from the SIDS project and building ML for healthcare.",html:"<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n    <title>Automating Healthcare Diagnostics</title>\n    <style>\n      body {\n        font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto,\n          Inter, \"Helvetica Neue\", Arial, \"Noto Sans\", \"Apple Color Emoji\",\n          \"Segoe UI Emoji\";\n        line-height: 1.65;\n        margin: 2rem auto;\n        max-width: 800px;\n        padding: 0 1rem;\n        color: #e0e0e0; /* Light gray for text */\n        background: #2a2a4a; /* Dark blue/purple background */\n      }\n      h1,\n      h3,\n      h3,\n      h4,\n      h5,\n      h6 {\n        color: #ffffff; /* White for headings */\n      }\n      a {\n        color: #87ceeb; /* Sky blue for links */\n      }\n      p {\n        margin: 0.9rem 0;\n      }\n      ul {\n        padding-left: 1.25rem;\n      }\n    </style>\n  </head>\n  <body>\n    <main class=\"doc\" role=\"main\">\n      <h1>Automating Healthcare Diagnostics</h1>\n      <h2 class=\"c12 subtitle\" id=\"h.6x6u16m6167t\">\n        <span>Lessons from the SIDS project</span>\n      </h2>\n\n      <p>\n        When I first dove into the project of predicting Sudden Infant Death\n        Sydrome (SIDS), I had no idea how deeply personal it would become. Our\n        capstone began in January 2024, the last full-time semester of my\n        master's at Rice University. At the same time, my wife and I were\n        preparing to welcome our first child — a baby girl, due right around\n        mid-April, just as I was wrapping up the semester and transitioning from\n        full-time student to full-time data scientist at my lab. The timing was\n        intentional. We had planned it carefully: I'd finish my graduate\n        coursework, deliver my final presentations, and step into fatherhood and\n        a full-time role with no academic weight holding me back.\n      </p>\n\n      <p>\n        But life, like machine learning models trained on noisy real-world data,\n        rarely behaves predictably. Our daughter was born prematurely, weeks\n        before we expected. As I worked through the most technically and\n        emotionally intense project of my academic career (building an AI\n        pipeline to predict sudden death in vulnerable infants), I was also\n        navigating neonatal care in real life, watching over a tiny human who\n        had arrived before she was ready. Each day, I would bounce between\n        debugging code for heart rate variability analysis and learning how to\n        feed, swaddle, and protect a medically fragile newborn.\n      </p>\n\n      <p>\n        That convergence made this project real. Not abstract. Not academic. It\n        was personal. I wasn't just thinking about AI for hypothetical babies in\n        the NICU. I was looking at mine, yearning to hold her in my arms. And\n        that experience sharpened my sense of responsibility in ways I can't\n        quite put into words.\n      </p>\n\n      <p>\n        This blog post explores how our capstone project (Team Breath of Life at\n        Rice University) harnessed AI to predict cardio-respiratory failure in\n        SIDS. More specifically, this post recounts our journey in building a\n        machine-learning pipeline to identify cardio-respiratory signatures of\n        SIDS in a mouse model. It's a story about data, yes — but also about\n        timing, leadership, life, and a different kind of early warning. Along\n        the way, I'll share how we engineered features that capture a\n        heartbeat's shape, trained neural networks on spectrogram “heatmaps,”\n        and tried to help machines learn what parents and doctors struggle to\n        catch in time.\n      </p>\n\n      <h3 id=\"understanding-the-sids-challenge\">\n        Understanding the SIDS Challenge\n      </h3>\n\n      <p>\n        SIDS has haunted new parents and doctors for generations. Coined in the\n        1970s, the term describes a diagnosis of exclusion where an infant\n        suddenly dies with no apparent cause. Most cases happen during sleep,\n        often when a baby is found face-down and believed to have re-breathed\n        CO₂-rich, oxygen-poor air. In a healthy infant, low oxygen would trigger\n        an auto-resuscitation reflex: the brainstem sounds an internal alarm,\n        making the baby gasp and “reset” breathing and heart rate. In SIDS, for\n        reasons still unknown, this protective reflex fails, and the infant\n        simply doesn't wake up.\n      </p>\n\n      <p>\n        Researchers have uncovered risk factors and theories. Babies sleeping in\n        unsafe positions or exposed to tobacco smoke during pregnancy have a\n        higher risk. Some hypotheses focus on subtle abnormalities in the\n        brainstem or the autonomic nervous system that impair breathing control.\n        Others point to genetic predispositions or cardiac arrhythmias. However,\n        an exact cause remains elusive since it's likely multi-factorial. What's\n        clear is that we cannot yet predict which infant will succumb to SIDS.\n        This unpredictability is what makes SIDS so terrifying and motivates\n        work on early detection.\n      </p>\n\n      <p>\n        Each year in the United States, over 1,300 infants die from SIDS. This\n        number has plateaued despite public health campaigns about safe sleep.\n        It would be invaluable if we could identify high-risk infants through\n        non-invasive monitoring (say, analyzing an infant's heartbeat or\n        breathing patterns). This is the vision of predictive healthcare in this\n        context: rather than just reacting to an emergency, we'd love an AI\n        “smoke alarm” that alerts caregivers before a baby's life is in danger.\n        Recent research is starting to move in this direction. For instance, a\n        2024 study identified metabolic biomarkers in blood that might help flag\n        infants at risk of SIDS. And in neonatal intensive care units (NICUs),\n        machine learning has been used on vital signs like heart rate\n        variability to warn of impending sepsis infections hours in advance.\n        These efforts all strive toward proactive, preventative monitoring as a\n        theme at the heart of our project.\n      </p>\n\n      <h3 id=\"from-mice-to-insights-the-sids-mouse-model\">\n        From Mice to Insights: The SIDS Mouse Model\n      </h3>\n\n      <p>\n        One big challenge in SIDS research is that you obviously cannot\n        experiment on human infants. To study SIDS triggers in a controlled way,\n        our collaborators at Baylor's Ray Lab turned to a mouse model. They\n        created a simulation of SIDS in the lab using newborn mouse pups. Each\n        mouse pup was placed in a chamber and fitted with tiny ECG leads to\n        record its heart activity, plus a custom face mask. Through the mask,\n        the system periodically gave the pup a gas mixture with no oxygen\n        (typically a nitrogen/carbon dioxide mix) to challenge its breathing.\n        This low-oxygen, high-CO₂ environment would cause the pup's heart rate\n        to drop and its breathing to cease, essentially inducing an apnea event\n        as seen in SIDS. The moment the pup stopped breathing, the system\n        returned normal air to the chamber, allowing it to gasp and recover if\n        it could (this is the auto-resuscitation reflex in action). The cycle\n        would repeat: challenge the pup with low oxygen, then rescue it, until,\n        eventually, in some unfortunate cases, the pup's reflex failed, and it\n        did not recover. In other words, the mouse succumbed to a SIDS-like\n        event.\n      </p>\n\n      <p>\n        Over several years, the Ray Lab conducted hundreds of these automated\n        experiments on mouse pups. Each experiment produced a rich collection of\n        data: continuous ECG waveforms (electrocardiogram signals) tracking the\n        heart's electrical activity, respiratory signals (from a small airflow\n        sensor in the mask), and timestamps of each apnea episode and outcome\n        (recovery or death). The lab's custom software, Breathe Easy, processed\n        the raw waveforms into a set of numeric features for each mouse. For\n        example, it extracted each “breath” and measured durations, and\n        similarly recorded each heartbeat's timing. By the time we, the Rice\n        capstone team, received the data, we had a database of 357 mice with\n        about 3,584 apnea events in total, each event labeled as either a normal\n        apnea (the mouse recovered) or a fatal apnea (the mouse died, e.g., SIDS\n        in this model).\n      </p>\n\n      <p>\n        As the only graduate student on the team (and the one with a biomedical\n        signals background), I took the lead on transforming this trove of\n        experimental data into something we could feed into machine learning\n        models. Our goal was straightforward to state: for each apnea event, use\n        the preceding ECG and breathing data to predict whether it will be fatal\n        or not. If our AI could learn the difference between a “recoverable”\n        apnea and a pre-SIDS apnea, it might reveal the hidden warning signs\n        that life itself failed to notice.\n      </p>\n\n      <h3 id=\"building-the-signal-processing-and-ml-pipeline\">\n        Building the Signal Processing and ML Pipeline\n      </h3>\n\n      <h3>Data Wrangling and Exploration:</h3>\n\n      <p>\n        I began by digging into the feature lists from Breathe Easy. These\n        included 177 parameters per apnea event, such as the apnea duration, the\n        breathing rate leading up to it, the average heart rate, etc. First, we\n        tried classical statistical analysis: were any of these 177 features\n        significantly different in SIDS-versus-normal events? We applied ANOVA\n        tests and Mann-Whitney U tests to compare groups. The result: no single\n        feature popped out as a clear discriminator. This was deflating at first\n        since we didn't find any simple “smoking gun”, like “heart rate drops by\n        X% only in the SIDS cases.”\n      </p>\n\n      <p>\n        We also trained a quick random forest classifier using a subset of these\n        features that our project sponsors (the Baylor doctors) suspected might\n        be important. This was essentially our baseline machine-learning attempt\n        on the raw feature set. Initially, the random forest just predicted the\n        majority class every time (e.g., “no SIDS event”), which isn't\n        surprising since there were far more recoveries than deaths (a heavy\n        class imbalance). We then adjusted the model's class weights to penalize\n        SIDS errors more, hoping to coax it into catching the rare SIDS case.\n        The outcome was still poor: the model would achieve a high overall\n        accuracy (because most events are non-fatal, and it got those right),\n        but it rarely actually predicted a SIDS event. In fact, its recall for\n        SIDS was near zero (around 8.4% by one analysis). In other words, it\n        might catch 1 out of 12 actual SIDS cases, missing all the rest, which\n        isn't a proper early warning system at all. This taught us our first\n        lesson: a model can appear “accurate” with imbalanced healthcare data\n        while completely failing the minority class of critical interest. SIDS\n        events were needles in a haystack, and the haystack was winning.\n      </p>\n\n      <p>\n        Frustrated by the flatness of the raw features, I decided to engineer\n        new features that might tease out hidden patterns. This is where domain\n        knowledge in signal processing became invaluable; we know from\n        physiology that subtle changes in heartbeats can indicate distress. For\n        example, researchers have long studied heart rate variability (HRV) in\n        infants, and some works suggested that infants who later succumb to SIDS\n        have “unique” variability patterns in their heart rate. Also, the shapes\n        of ECG waveforms (the QRS complexes) could hold clues about cardiac\n        function. So, we expanded our feature set in a few key ways:\n      </p>\n\n      <ul>\n        <li>\n          <span class=\"smallcaps\">Heartbeat Morphology:</span> We broke down\n          each QRS complex (the spike in the ECG for each heartbeat) into its\n          constituent waves: Q, R, and S peaks. We computed its amplitude\n          (height in mV) for each wave and how that amplitude changed compared\n          to the previous heartbeat. Subtle shifts in these amplitudes might\n          reveal deterioration in cardiac output. We also measured timing\n          intervals within each heartbeat: the time between Q and R, R and S,\n          and Q and S. These within-complex intervals, along with the between\n          complex interval (the gap between successive R-waves, e.g., the R–R\n          interval), quantify how regular or irregular the heartbeat timing is.\n          Notably, R–R interval variability is a classic HRV measure, which is\n          important in infant cardiorespiratory health. Finally, inspired by\n          signal-processing research, we calculated the area under the QRS curve\n          for each beat as a proxy for morphological shape. Prior studies have\n          used similar area-based metrics to classify abnormal heartbeats like\n          arrhythmias, so it was a promising feature to include.\n        </li>\n\n        <li>\n          <span class=\"smallcaps\">Heart Rate Dynamics:</span> Instead of just\n          looking at the instantaneous heart rate or average, we examined how\n          the heart rate fluctuated over time leading up to an apnea. I\n          implemented a Katz Fractal Dimension (KFD) calculation on the heart\n          rate time series. This gave us a single number representing the\n          complexity of the heart rate signal, where a higher fractal dimension\n          means a more erratic, less predictable pattern. The intuition is that\n          a distressed or unstable physiological state might produce more\n          complex, chaotic heart rate fluctuations. (Think of a calm, healthy\n          baby's heart versus one that's struggling; one might expect different\n          patterns of variability.) We also considered entropy-based metrics\n          (like fuzzy entropy) to quantify signal irregularity since these have\n          been used in biomedical signal analysis to capture complexity in noisy\n          data. These metrics complement traditional HRV measures by\n          highlighting nonlinear and non-obvious patterns.\n        </li>\n\n        <li>\n          <span class=\"smallcaps\">Respiratory Features:</span> The breathing\n          waveform, too, held potential clues. The original 177 features (the\n          “breathlist”) came mainly from the respiration signal, but since none\n          were individually predictive, we brainstormed combinations or\n          transformations. For instance, we looked at sequences of breaths prior\n          to the apnea: was there a progressive slowing of breathing? An\n          increase in variability of breath amplitude? One idea from the\n          literature was to examine respiratory entropy or complexity, similar\n          to the ECG approach, since irregular breathing patterns might precede\n          a failure. We also derived features capturing the interaction between\n          heart and breathing signals; for example, the coupling between heart\n          rate and respiration (a phenomenon known as respiratory sinus\n          arrhythmia). These are more exploratory, but the goal was to arm our\n          models with as detailed a description of the pre-apnea state as\n          possible, since we didn't know what subtle combination might be the\n          harbinger of collapse.\n        </li>\n      </ul>\n\n      <p>\n        All told, by the end of feature engineering, we had 200+ features for\n        each apnea event, spanning classic vital sign stats to exotic nonlinear\n        metrics. We were throwing the kitchen sink at the problem, but in a\n        principled way grounded in cardio-respiratory physiology. I often\n        toggled between two mindsets: the data scientist tweaking code to\n        calculate features and the biomedical engineer asking, “Does this\n        feature make sense for what an infant's body is doing?”. This dual\n        approach was crucial; it's a lesson I carry forward: in healthcare AI,\n        blending domain insight with data-driven methods amplifies the power of\n        both.\n      </p>\n\n      <h3>Visual EDA – A Spectrogram Surprise:</h3>\n\n      <p>\n        I also performed visual exploratory data analysis while computing these\n        features. One technique that made a significant impression on our team\n        was plotting spectrograms of the ECG signals. A spectrogram turns a time\n        series signal into an image, showing how the signal's frequency content\n        changes over time (via Short-Time Fourier Transform, or STFT for short).\n        I took ECG data from each mouse and generated spectrogram images for the\n        5-minute window leading up to an apnea event. In short, the x-axis was\n        time, the y-axis was frequency (0.5–50 Hz band of interest), and pixel\n        intensity showed signal power at each frequency. Then, I compared\n        spectrograms for two scenarios: before a normal apnea (mouse survived)\n        vs. before a fatal apnea (mouse died). The difference was striking: the\n        ECG spectrograms before fatal apneas looked smoother and more regular,\n        while those before recovery apneas looked choppier and noisier. Figure 7\n        in our report captured this pattern clearly, where the “at-risk” ECG had\n        a kind of eerie calm, while the recovery-bound ECG was more erratic.\n        This was a big clue. It's suggested that whatever breakdown leads to\n        SIDS might manifest as a loss of variability or loss of complexity in\n        the heart signal shortly before the event. In other words, the\n        spectrogram visualized some of the same phenomena we hoped our features,\n        like fractal dimension, would quantify.\n      </p>\n\n      <p>\n        This discovery shaped our strategy. If the difference is visible to the\n        eye in a spectrogram, then a computer vision model could also learn it.\n        We decided to pursue two parallel modeling approaches from here on: one\n        leveraging our carefully engineered numerical features (many of which\n        tried to capture the kind of variability differences we saw) and another\n        leveraging the raw spectrogram images directly with deep learning. It\n        was time to move from data exploration to building predictive models.\n      </p>\n\n      <h3 id=\"two-modeling-approaches\">\n        Two Modeling Approaches: Features vs. Spectrograms\n      </h3>\n\n      <p>\n        In modern AI, there's a bit of a dichotomy: feed the algorithm carefully\n        curated features, or feed it raw data and let it figure out the features\n        itself. We decided to try both.\n      </p>\n\n      <ol>\n        <li>\n          <strong>Feature-Based Time-Series Model (1D CNN + LSTM):</strong> I\n          built a neural network that could process a sequence of feature\n          vectors over time for the tabular dataset of engineered features. Each\n          apnea event wasn't just a single timestamp – we had a time series\n          leading up to it (e.g., 10 seconds or 5 minutes of data, depending on\n          the window, which we could break into smaller sub-intervals). To model\n          temporal patterns in the features (like trends or oscillations in\n          heart rate), we used a hybrid architecture: a 1D Convolutional Neural\n          Network followed by a Long Short-Term Memory (LSTM) network. The 1D\n          CNN component acted as an automated feature extractor across the\n          sequence (for example, spotting a spike or drop in some metric), and\n          the LSTM gave the model a “memory” to connect patterns across time.\n          Essentially, this network looks at a rolling window of our features\n          and learns an internal representation of the cardio-respiratory\n          dynamics before predicting SIDS vs non-SIDS at the end. We trained\n          this model on all our events, taking care to do proper\n          cross-validation given the limited data. (With only ~3.5k events, we\n          had to be wary of overfitting; I used techniques like dropout\n          regularization and limited the network size accordingly.)\n        </li>\n\n        <li>\n          <strong\n            >Image-Based Deep Learning Model (2D CNN on Spectrograms):</strong\n          >\n          In parallel, we treated each event's ECG (and potentially breathing)\n          spectrogram as an image and trained a convolutional neural network to\n          classify it. If a human can glance at the spectrogram and notice\n          “smoother vs noisier” patterns, a CNN should also be able to learn the\n          nuanced differences. We used a straightforward 2D CNN architecture\n          (inspired by standard image classifiers with convolutional layers,\n          pooling, etc., ending in a softmax for two classes). Here, each\n          training example was a spectrogram image labeled “SIDS” or “non-SIDS.”\n          To augment the data (since 3,584 images is not a lot by deep learning\n          standards), we did some basic augmentation like flipping or adding\n          slight jitter to the images, and we were careful to filter frequencies\n          to the same range for all (0.5–50 Hz) to reduce noise. The CNN's job\n          was to implicitly learn features like “lack of high-frequency\n          variation” or “presence of certain oscillatory patterns” that might\n          correlate with impending SIDS. This approach essentially lets the data\n          speak with a less human preconception since the network might discover\n          a pattern we didn't hypothesize.\n        </li>\n      </ol>\n\n      <p>\n        Both approaches were implemented in Python (PyTorch for the CNN models),\n        and we managed our experiments in the project's GitHub repository. As\n        the technical lead, I wrote most of this modeling code and spent long\n        nights tuning hyperparameters, but team collaboration was key in\n        interpreting results and deciding the next steps. For example, when one\n        model would make an incorrect prediction, we'd examine that case\n        together to see if there was a biomedical reason or just noise.\n      </p>\n\n      <h3 id=\"results-sensitivity-over-accuracy\">\n        Results: Sensitivity Over Accuracy\n      </h3>\n\n      <p>\n        After training and testing, we had a tale of two models. The\n        feature-based CNN+LSTM achieved an overall accuracy of around 91%, which\n        was initially a cause for celebration. Nevertheless, my heart sank when\n        I dug into the confusion matrix: the model had effectively learned to\n        predict “no SIDS” for every case. It wasn't truly 91% accuracy because\n        it was smart; it was just taking advantage of the class imbalance. It\n        failed to catch any of the SIDS events. In other words, it had zero\n        sensitivity (recall) for the positive class. We had seen this pattern\n        before with the random forest; now, even a more complex model fell into\n        the trap. Despite our sophisticated features, the network likely found\n        it safer (in terms of minimizing loss) to ignore the rare positives and\n        focus on getting the majority right. This underscores how tricky\n        imbalanced medical data can be. We did try techniques to mitigate this\n        (e.g., class weight adjustments in the loss function, resampling), but\n        the bottom line was that this model was not viable if it wouldn't raise\n        the alarm for an actual SIDS event. A 91% accuracy is meaningless if the\n        9% it gets wrong are the only cases you care about.\n      </p>\n\n      <p>\n        On the other hand, the spectrogram CNN told a more promising story. Its\n        accuracy was lower (about 86% on the test set), but importantly, it did\n        catch some SIDS cases. In fact, its recall (sensitivity) for SIDS was\n        ~0.45 (45%). It wasn't anywhere near perfect, but this was an ample\n        improvement from 0%. The precision (positive predictive value) was about\n        50.6%, meaning roughly half of the events flagged as “SIDS likely” were\n        actual SIDS cases. To put it plainly, the CNN would correctly identify\n        ~45% of impending SIDS events before they happened, at the cost of some\n        false alarms (for every event it got right, it also misidentified one\n        that turned out fine). This trade-off (more false positives but catching\n        some true positives) is precisely what we wanted in this context. Given\n        the life-or-death nature of SIDS, it is far better to have an alarm that\n        cries wolf occasionally than one that sleeps through the real danger. We\n        prioritized sensitivity over specificity. In clinical terms, missing a\n        SIDS event (a false negative) is the worst outcome; a false positive\n        alarm that wakes the parent unnecessarily is a nuisance but not a\n        tragedy.\n      </p>\n\n      <p>\n        The confusion matrix for the CNN reflected this balance: it had a\n        meaningful number of true positives (SIDS predicted correctly) and some\n        false positives, whereas the feature model had essentially no true\n        positives. This result validated our hypothesis that the\n        frequency-domain patterns carried predictive signals that the\n        human-crafted features weren't fully capturing. The CNN learned to\n        notice something in the spectrogram, perhaps a loss of high-frequency\n        heart rate variability or a particular respiratory oscillation, that\n        correlates with failure to auto-resuscitate. It's intriguing to think\n        what exactly it's keying in on. Is it essentially measuring heart rate\n        variability in its own convolutional way? Is it picking up on a slow\n        drift or electrical stability in the heart prior to failure? Deep\n        learning is often criticized as a “black box.” However, we plan to apply\n        interpretation techniques (like saliency maps on the spectrograms) in\n        the future to reverse-engineer the features that the CNN found\n        important.\n      </p>\n\n      <h3>Key takeaway from results:</h3>\n\n      <p>\n        In AI for healthcare, the success metric depends on context. Our two\n        models demonstrated this vividly. If you only looked at accuracy, you'd\n        pick the 91% model and be utterly wrong in practice. You must consider\n        what matters: here it was catching that one critical event. In fact, our\n        final recommendation was to favor the spectrogram CNN despite its lower\n        accuracy because it was more likely to predict SIDS when it was\n        genuinely going to occur. This lesson aligns with a broader trend in\n        medical AI: you tune the system not for vanity metrics but for the\n        outcome that saves lives. Sometimes, that means accepting more false\n        alarms. A parallel can be found in NICUs with sepsis alarms built from\n        algorithms that monitor infants' vital signs dramatically reduce\n        neonatal sepsis mortality by alerting staff earlier, even though they\n        aren't perfectly accurate. We envisioned our SIDS predictor in a similar\n        light: a somewhat noisy alarm is far better than silence.\n      </p>\n\n      <p>\n        We did face the limitation of small data. With only a few thousand\n        training examples and a deep CNN with ~24 million parameters, we knew\n        overfitting was a risk. In fact, by the rule of thumb, one often wants\n        10× more data points than parameters for robust learning, which is an\n        impossible 240 million data points in our case. We mitigated this by\n        regularization and leveraging domain constraints (frequency filtering,\n        limited input window, etc.), but scaling up the dataset size is\n        essential. One lesson here is that data is often the scarcest resource\n        in biomedical projects. We were fortunate to have any SIDS examples at\n        all (since it's hard to acquire). However, future studies will need to\n        gather more, possibly by running more mouse experiments or by collecting\n        analogous physiological data from human infants who had apparent\n        life-threatening events (ALTEs) or non-fatal apneas.\n      </p>\n\n      <h3 id=\"leading-a-multidisciplinary-effort-personal-reflections\">\n        Leading a Multidisciplinary Effort – Personal Reflections\n      </h3>\n\n      <p>\n        On a personal note, this project was as much about people as it was\n        about technology. As the technical lead and sole grad student on a team\n        of undergraduates, I wore many hats: project manager, data engineer,\n        machine learning scientist, and occasional domain translator. I guided\n        the team through brainstorming features, taught newer members about\n        concepts like Fourier transforms and neural networks, and kept us on a\n        rigorous schedule to meet deliverables. One day, I'd be debugging Python\n        code to fix an ECG signal preprocessing bug; the next day, I'd present\n        our latest findings to pediatricians and biologists, explaining what a\n        “spectrogram CNN” is in lay terms. This taught me the importance of\n        communication across disciplines. We had brilliant domain experts at\n        Baylor providing us with context on the biology of SIDS, and it was my\n        job to ensure we correctly translated their knowledge into our data\n        features and that we translated our results back into meaningful\n        biomedical insights.\n      </p>\n\n      <p>\n        A concrete example of this synergy was when the doctors suggested\n        certain features (like specific breathing characteristics) based on\n        their experience. Even though our initial inclusion of the entire\n        breathlist didn't yield a silver bullet, those conversations sparked\n        ideas for feature engineering (such as looking at variability in\n        breathing patterns). In turn, when our models started working, I created\n        visualizations of what the model was seeing, like overlaying the\n        “smoother” versus “noisier” ECG signals, to discuss with the doctors.\n        That moment when a neonatologist nods and says, “Interesting, that makes\n        sense physiologically,” you feel the gap between AI and medicine narrow\n        just a little.\n      </p>\n\n      <p>\n        From a leadership perspective, I learned how to balance innovation with\n        pragmatism. It's easy to get excited about fancy deep learning models\n        (guilty as charged), but part of my role was ensuring we also tried\n        simpler approaches and didn't overlook simpler insights. We did, for\n        instance, spend time on classical statistical tests and a baseline\n        random forest, which isn't glamorous, but it established a point of\n        comparison and justified the need for more complex modeling when those\n        failed. Since multiple people were contributing, I also helped manage\n        version control and coding standards in our GitHub repo. Enforcing good\n        practices early (like clear documentation and unit tests for our data\n        preprocessing functions) saved us from chaos later. These multifaceted\n        skills are the sorts of project management lessons that one doesn't\n        always learn in class but are critical in real-world tech teams.\n      </p>\n\n      <p>\n        Finally, being the lead on a high-stakes healthcare project impressed\n        upon me the ethical responsibility we carry. We often discussed what a\n        “prediction” really means – if our model alerts a SIDS risk, how\n        confident can we be? What should the response be, wake the baby? Take it\n        to a hospital? False positives have consequences (parent anxiety,\n        over-treatment), but false negatives have the worst consequence of all.\n        This prediction isn't just an academic exercise; it's potentially life\n        and death. That kept us intellectually honest and cautious in how we\n        presented our findings. We were careful not to over-hype the results and\n        emphasized that more validation is needed before anyone trusts an AI\n        with a baby's life. I think this mindset is crucial for anyone looking\n        to bring AI into healthcare: accuracy metrics are just the start of the\n        conversation, not the end.\n      </p>\n\n      <h3 id=\"towards-real-time-neonatal-monitoring-future-directions\">\n        Towards Real-Time Neonatal Monitoring: Future Directions\n      </h3>\n\n      <p>\n        Our capstone project was a prototype and a proof of concept that AI can\n        pick up patterns in physiological signals that might precede SIDS. But,\n        turning this into a real-world diagnostic tool will require further\n        research and development. Here's where we see it heading:\n      </p>\n\n      <ul>\n        <li>\n          <strong>Real-Time Monitoring:</strong> The next step (already in\n          progress with our collaborators) is to test the spectrogram CNN on\n          real-time streaming data. In the mouse lab, this means running the\n          algorithm during live experiments to see if it can alert the\n          experimenters that a pup is about to succumb, potentially even\n          allowing an intervention. In human infants, real-time means analyzing\n          data from monitors on the fly in the crib or NICU. This\n          interactability poses challenges: our model would need to be efficient\n          and robust to noise and motion artifacts, and it would have to decide\n          continuously when to raise the alarm. We may need to combine\n          short-term and long-term analyses (for instance, checking a 5-minute\n          window every 30 seconds, etc.). The code we wrote is refactored for\n          low-latency inference to enable this streaming prediction.\n        </li>\n\n        <li>\n          <strong>Integrated Multi-Modal Signals:</strong> Our project focused\n          on ECG and respiration data, but infants in NICUs often have other\n          monitors like pulse oximetry (oxygen levels), temperature, and even\n          video. A future system might integrate multiple data sources for a\n          more confident prediction. Interestingly, the recent study identifying\n          metabolic biomarkers for SIDS hints that researchers can combine\n          biochemical signals (like specific proteins or metabolites in the\n          blood) with physiological signals. Perhaps an AI system could one day\n          synthesize genetic risk factors, metabolic indicators, and real-time\n          vital signs into a single risk score for SIDS. This platform is still\n          speculative, but it aligns with the direction of personalized,\n          predictive medicine.\n        </li>\n\n        <li>\n          <strong>Better Algorithms for Imbalanced Data:</strong> We got a crash\n          course in dealing with class imbalance, but there's room for more\n          advanced techniques. Future models could employ anomaly detection\n          framing by treating SIDS events as anomalies to detect rather than as\n          one-half of a balanced classification. There's promising research on\n          training models to learn what “normal” looks like and then flagging\n          deviations. In fact, one could imagine training on the abundant\n          non-SIDS data (apneas where recovery occurred) to establish a baseline\n          and using unsupervised or semi-supervised methods to spot the oddball\n          cases. Recent advances in time-series anomaly detection (2023–2025)\n          could be applicable here, ensuring the model doesn't just learn to be\n          a majority vote predictor.\n        </li>\n\n        <li>\n          <strong>Scaling and Generalization:</strong> Ultimately, to have\n          confidence in such a system, we need to test it on larger and more\n          diverse datasets. That could mean more mouse experiments (perhaps\n          including induced SIDS in mice with specific genetic mutations to see\n          if the model picks up genotype-specific patterns). More vitally, it\n          would mean collecting data from infant monitors in hospitals. One idea\n          is to retrospectively analyze monitor data from infants who had ALTEs\n          (Apparent Life-Threatening Events) or who researchers monitored due to\n          being siblings of SIDS victims to see if our model's patterns hold up.\n          These ideas are easier said than done due to privacy and data\n          availability, but it's the direction to move before clinicians can\n          deploy such a model.\n        </li>\n\n        <li>\n          <strong>Towards a Smart Baby Monitor:</strong> The moonshot vision\n          motivated us throughout as a consumer-friendly device that could be in\n          a crib at home, silently watching over a sleeping infant. Imagine a\n          baby monitor or a wearable sock that not only tracks heart rate and\n          breathing (some products do that already) but runs an AI algorithm\n          trained to recognize the red-flag pattern of impending SIDS. If\n          detected, it could alert parents or stimulate the baby (some have\n          proposed vibrating devices that nudge a baby if they stop breathing).\n          Our report explicitly noted that integrating these findings into\n          consumer health tech is a goal. As someone moving toward industry, I\n          see a startup opportunity here, but there is also a need for rigorous\n          clinical testing and regulatory approval, given the stakes. Any such\n          device would likely need FDA clearance and careful risk-benefit\n          analysis. It's a challenging road, but the lives saved each year could\n          be well worth it.\n        </li>\n      </ul>\n\n      <h3 id=\"conclusion-lessons-learned-and-future-outlook\">\n        Conclusion: Lessons Learned and Future Outlook\n      </h3>\n\n      <p>\n        The SIDS AI project was a capstone in every sense for me: technically,\n        educationally, and personally. We started with a heartbreaking problem\n        and a heap of messy data and extracted a glimmer of insight and hope.\n        Along the way, I learned a few key lessons:\n      </p>\n\n      <ul>\n        <li>\n          <strong>Marry Domain Knowledge with AI Creativity:</strong> Neither\n          alone would have succeeded. Our project sponsors' deep understanding\n          of neonatal physiology guided our feature engineering (e.g., focusing\n          on heart rate variability and respiratory patterns) to improve our\n          models' chances. Conversely, letting cutting-edge AI loose on\n          spectrograms revealed patterns no one might have coded as a feature\n          (smooth vs choppy signal textures). This marriage of approaches\n          yielded results where each alone had failed. I've come to appreciate\n          that in fields like healthcare, you can't treat AI as a magic black\n          box; you need to guide it with what you know about the science.\n        </li>\n\n        <li>\n          <strong\n            >Prioritize the Clinical Objective, Not Just the Metric:</strong\n          >\n          Our initial accuracy highs were misleading. The real goal was a high\n          recall of events, even at the expense of false positives. This is a\n          common theme in diagnostic medicine: a screening test should catch all\n          potential issues, and a follow-up test can weed out false alarms. So,\n          we basically built a screening tool. Keeping that end-use in mind (and\n          the value of a life saved versus an inconvenience) helped us set the\n          correct targets and correctly choose the CNN model that aligned with\n          those targets.\n        </li>\n\n        <li>\n          <strong\n            >Small Data Is a Big Problem (But You Can Get Creative):</strong\n          >\n          Working with limited data forced us to be clever with techniques and\n          careful not to overfit. We augmented data, used transfer learning\n          ideas from image recognition, and did extensive cross-validation.\n          Still, there's no substitute for more data. This challenge is a lesson\n          for anyone working on a similar project: if you can collect more, do\n          it. If you can't, acknowledge the limitations and design your model\n          evaluation accordingly (we reported confidence intervals and avoided\n          over-claiming performance). Encouragingly, even small datasets can\n          yield meaningful models in this age, especially if combined with\n          knowledge-based feature design, but robustness will always be a\n          concern until validated on larger scales.\n        </li>\n\n        <li>\n          <strong>Interdisciplinary Communication is Key:</strong> I had to\n          learn to speak the language of doctors and biologists, and they had to\n          learn a bit of mine. This mutual education meant that, for example,\n          when our model found something, the physicians on the team could\n          interpret it in terms of autonomic nervous function or hypoxia\n          tolerance. That's when AI becomes more than numbers; it becomes\n          insight. I suspect the future of AI in healthcare will see many of\n          these partnerships, and success will depend on our ability as\n          technologists to collaborate and communicate beyond our field.\n        </li>\n      </ul>\n\n      <p>\n        As I finish writing this, I'm struck by how far we've come and how much\n        there is to go. Automating healthcare diagnostics with AI is not an\n        overnight revolution but a gradual integration, solving one problem at a\n        time, one project at a time. Our SIDS project was one such step, turning\n        raw signals into a hint of foresight. Standing at the interface of data\n        science and medicine, I'm optimistic. The future of predictive\n        healthcare is emerging now: from NICU algorithms that anticipate infant\n        sepsis, to wearable ECG devices that warn of heart attacks before the\n        patient feels anything, to perhaps one day a crib monitor that whispers\n        an alert and prevents a crib death. These advances are born out of\n        multidisciplinary teams and persistent iteration, just like ours.\n      </p>\n\n      <p>\n        In the end, our AI didn't solve SIDS (that day is still ahead, we hope),\n        but it taught us lessons about what it will take. It reinforced my\n        passion for this field: the chance to save even one life with code and\n        ingenuity is a powerful motivator. On a personal level, leading this\n        capstone showed me the kind of impact a determined engineer can have in\n        healthcare with the right team, the right tools, and a whole lot of\n        heart.\n      </p>\n\n      <h3 id=\"references-and-further-reading\">\n        References and Further Reading\n      </h3>\n\n      <ul class=\"refs\">\n        <li>\n          Beckwith, J.B. (1973). The sudden infant death syndrome.\n          <em>Current Problems in Pediatrics</em>, 3(8), 3–36.\n        </li>\n\n        <li>\n          Huang, Y. et al. (2023). Statistical report on SIDS incidence (~1,300\n          annual cases in the US).\n        </li>\n\n        <li>\n          Hewitt, A.L. et al. (2020). Study on heart rate variability\n          differences in infants who succumbed to SIDS vs. controls.\n        </li>\n\n        <li>\n          Nezamabadi, M. et al. (2022). Research on ECG R-wave feature\n          engineering for infant risk assessment.\n        </li>\n\n        <li>\n          Popescu, A. et al. (2009). Use of QRS complex morphological\n          integration (area under the curve) for classifying abnormal\n          heartbeats.\n        </li>\n\n        <li>\n          Keles, E., &amp; Bagci, U. (2023). The past, current, and future of\n          NICUs with AI: a systematic review. <em>npj Digital Medicine</em>,\n          6(220). (Summary of 106 studies on AI in neonatology; highlights use\n          of vital sign analysis and need for data-driven early warning\n          systems.)\n        </li>\n\n        <li>\n          Oltman, S. et al. (2024). Metabolic biomarkers and SIDS risk.\n          <em>JAMA Pediatrics</em>. (Identified metabolic signals in infants who\n          died of SIDS, suggesting new avenues for risk prediction.)\n        </li>\n\n        <li>\n          (Code Repository) RiceD2KLab/BCM_SIDS_Sp24 – Utilizing Machine\n          Learning to Identify Cardio-Respiratory Signatures Predictive of SIDS.\n          (All project code and models are privately available on GitHub upon\n          request).\n        </li>\n      </ul>\n\n      <hr />\n\n      \x3c!-- <h3>Outline</h3>\n      <ul>\n        <li>Understanding the SIDS Challenge</li>\n        <li>From Mice to Insights: The SIDS Mouse Model</li>\n        <li>Building the Signal Processing and ML Pipeline</li>\n        <li>Data Wrangling and Exploration:</li>\n        <li>Visual EDA – A Spectrogram Surprise:</li>\n        <li>Two Modeling Approaches: Features vs. Spectrograms</li>\n        <li>Results: Sensitivity Over Accuracy</li>\n        <li>Key takeaway from results:</li>\n        <li>Leading a Multidisciplinary Effort – Personal Reflections</li>\n        <li>Towards Real-Time Neonatal Monitoring: Future Directions</li>\n        <li>Conclusion: Lessons Learned and Future Outlook</li>\n        <li>References and Further Reading</li>\n      </ul> --\x3e\n    </main>\n  </body>\n</html>\n"},"segmenting-the-invisible":{title:"Segmenting the Invisible | cargonriv.com Blog",description:"SAM, DINO, and the future of image analysis.",html:'<!DOCTYPE html>\n<html>\n  <head>\n    <title>Segmenting the Invisible</title>\n    <meta content="text/html; charset=UTF-8" http-equiv="content-type" />\n    <style type="text/css">\n      .lst-kix_ehzt5islygnw-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_pq6l9j27g4h8-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_ehzt5islygnw-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ehzt5islygnw-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_pq6l9j27g4h8-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_pq6l9j27g4h8-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_pq6l9j27g4h8-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_pq6l9j27g4h8-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_ehzt5islygnw-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_ehzt5islygnw-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_pq6l9j27g4h8-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_pq6l9j27g4h8-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_pq6l9j27g4h8-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ajkvb22l6j96-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ehzt5islygnw-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ehzt5islygnw-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_ikcljutuvuve-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_ehzt5islygnw-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-5 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_ajkvb22l6j96-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ehzt5islygnw-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-7 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ikcljutuvuve-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ehzt5islygnw-6 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-1 {\n        list-style-type: none;\n      }\n      .lst-kix_ajkvb22l6j96-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ajkvb22l6j96-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      ul.lst-kix_ehzt5islygnw-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-1 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-2 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-2 {\n        list-style-type: none;\n      }\n      .lst-kix_ajkvb22l6j96-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_ajkvb22l6j96-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ikcljutuvuve-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_ajkvb22l6j96-5 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      ul.lst-kix_ikcljutuvuve-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-7 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ikcljutuvuve-6 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-6 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ikcljutuvuve-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ikcljutuvuve-7 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ikcljutuvuve-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ikcljutuvuve-5 {\n        list-style-type: none;\n      }\n      .lst-kix_ehzt5islygnw-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ikcljutuvuve-2 {\n        list-style-type: none;\n      }\n      .lst-kix_ajkvb22l6j96-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_ikcljutuvuve-3 {\n        list-style-type: none;\n      }\n      .lst-kix_ehzt5islygnw-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_ikcljutuvuve-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ikcljutuvuve-1 {\n        list-style-type: none;\n      }\n      .lst-kix_ajkvb22l6j96-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_ajkvb22l6j96-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_thjubi60nqtj-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-2 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-1 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-7 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-6 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-5 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-4 {\n        list-style-type: none;\n      }\n      .lst-kix_thjubi60nqtj-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_thjubi60nqtj-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_pq6l9j27g4h8-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-5 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-6 {\n        list-style-type: none;\n      }\n      .lst-kix_thjubi60nqtj-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_pq6l9j27g4h8-0 {\n        list-style-type: none;\n      }\n      li.li-bullet-0:before {\n        margin-left: -18pt;\n        white-space: nowrap;\n        display: inline-block;\n        min-width: 18pt;\n      }\n      .lst-kix_thjubi60nqtj-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_thjubi60nqtj-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_pq6l9j27g4h8-1 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-2 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-7 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-8 {\n        list-style-type: none;\n      }\n      .lst-kix_thjubi60nqtj-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_thjubi60nqtj-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_thjubi60nqtj-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_thjubi60nqtj-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_pq6l9j27g4h8-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      ol {\n        margin: 0;\n        padding: 0;\n      }\n      table td,\n      table th {\n        padding: 0;\n      }\n      .c8 {\n        border-right-style: solid;\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        border-right-width: 0pt;\n        padding-left: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.38;\n        border-left-width: 0pt;\n        border-top-style: solid;\n        border-left-style: solid;\n        border-bottom-width: 0pt;\n        border-bottom-style: solid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n        padding-right: 0pt;\n      }\n      .c4 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.56;\n        border-top-style: solid;\n        margin-left: 51pt;\n        text-indent: -18pt;\n        border-bottom-width: 0pt;\n        border-bottom-style: solid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n        height: 11pt;\n      }\n      .c6 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        padding-left: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.56;\n        border-top-style: solid;\n        margin-left: 51pt;\n        border-bottom-width: 0pt;\n        border-bottom-style: solid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      .c13 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.56;\n        border-top-style: solid;\n        border-bottom-width: 0pt;\n        border-bottom-style: solid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n        height: 11pt;\n      }\n      .c17 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        border-bottom-width: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.38;\n        border-bottom-style: solid;\n        orphans: 2;\n        border-top-style: solid;\n        widows: 2;\n        text-align: left;\n      }\n      .c11 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        border-bottom-width: 0pt;\n        padding-bottom: 13.5pt;\n        line-height: 1.26;\n        border-bottom-style: solid;\n        orphans: 2;\n        border-top-style: solid;\n        widows: 2;\n        text-align: center;\n      }\n      .c12 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        border-bottom-width: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.56;\n        border-bottom-style: solid;\n        orphans: 2;\n        border-top-style: solid;\n        widows: 2;\n        text-align: center;\n      }\n      .c18 {\n        padding-top: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.15;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n        height: 11pt;\n      }\n      .c15 {\n        font-weight: 400;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 26pt;\n        font-family: "Arial";\n        font-style: normal;\n      }\n      .c9 {\n        font-weight: 400;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 17pt;\n        font-family: "Arial";\n        font-style: normal;\n      }\n      .c3 {\n        font-weight: 400;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 11pt;\n        font-family: "Arial";\n        font-style: normal;\n      }\n      .c2 {\n        font-weight: 400;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 12pt;\n        font-family: "Arial";\n        font-style: normal;\n      }\n      .c10 {\n        font-weight: 400;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 11pt;\n        font-family: "Arial";\n      }\n      .c5 {\n        background-color: #ffffff;\n        max-width: 468pt;\n        padding: 72pt 72pt 72pt 72pt;\n      }\n      .c0 {\n        color: #0e101a;\n        font-style: italic;\n      }\n      .c16 {\n        padding: 0;\n        margin: 0;\n      }\n      .c1 {\n        color: #0e101a;\n      }\n      .c7 {\n        font-weight: 700;\n      }\n      .c14 {\n        color: #000000;\n      }\n      .title {\n        padding-top: 0pt;\n        color: #000000;\n        font-size: 26pt;\n        padding-bottom: 3pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      .subtitle {\n        padding-top: 0pt;\n        color: #666666;\n        font-size: 15pt;\n        padding-bottom: 16pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      li {\n        color: #000000;\n        font-size: 11pt;\n        font-family: "Arial";\n      }\n      p {\n        margin: 0;\n        color: #000000;\n        font-size: 11pt;\n        font-family: "Arial";\n      }\n      h1 {\n        padding-top: 20pt;\n        color: #000000;\n        font-size: 20pt;\n        padding-bottom: 6pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h2 {\n        padding-top: 18pt;\n        color: #ffffff;\n        font-size: 16pt;\n        padding-bottom: 6pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h3 {\n        padding-top: 16pt;\n        color: #434343;\n        font-size: 14pt;\n        padding-bottom: 4pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h4 {\n        padding-top: 14pt;\n        color: #666666;\n        font-size: 12pt;\n        padding-bottom: 4pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h5 {\n        padding-top: 12pt;\n        color: #666666;\n        font-size: 11pt;\n        padding-bottom: 4pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h6 {\n        padding-top: 12pt;\n        color: #666666;\n        font-size: 11pt;\n        padding-bottom: 4pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        font-style: italic;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n    </style>\n  <style>\n    body.c5 {\n        background-color: #0e101a;\n    }\n    .title, .subtitle, h1, h2, h3, h4, h5, h6, p, li, span {\n        color: #ffffff !important;\n    }\n    .title {\n        font-size: 36pt !important;\n        text-align: center !important;\n    }\n    .subtitle {\n        font-size: 24pt !important;\n        text-align: center !important;\n    }\n    h1 { font-size: 30pt !important; }\n    h2 { font-size: 24pt !important; }\n    h3 { font-size: 20pt !important; }\n    h4 { font-size: 18pt !important; }\n    h5 { font-size: 16pt !important; }\n    h6 { font-size: 14pt !important; }\n    p {\n        margin-bottom: 1em;\n    }\n</style>\n</head>\n  <body class="c5 doc-content">\n    <h1 class="c11 title" id="h.iq1b57b6irdh">\n      <span class="c14 c15">Segmenting the Invisible</span>\n    </h1>\n    <h2 class="c12 subtitle" id="h.6x6u16m6167t">\n      <span>SAM, DINO, and the Future of Image Analysis</span>\n    </h2>\n    <h3 class="c17" id="h.1sob6urgak60">\n      Introduction<span class="c9 c1"></span>\n    </h2>\n    <p class="c8">\n      <span class="c1"\n        >In biological imaging, we often deal with the &quot;invisible&quot;:\n        microscopic cells, bacterial colonies, or subtle patterns that evade\n        easy detection. Traditional image analysis required painstaking tuning\n        of algorithms or training models from scratch on limited data. Today, a\n        new wave of </span\n      ><span class="c1 c7">foundation models</span\n      ><span class="c1">&nbsp;promises to change that. Meta&#39;s </span\n      ><span class="c1 c7">Segment Anything Model (SAM)</span\n      ><span class="c1"\n        >&nbsp;and related vision transformers, abbreviated as the ViT term like </span\n      ><span class="c1 c7">DINO</span\n      ><span class="c1"\n        >&nbsp;(self-distillation with no labels), are generalist vision models\n        trained on massive data. They can </span\n      ><span class="c1 c7">zero-shot segment or describe objects</span\n      ><span class="c1"\n        >&nbsp;in images without prior task-specific training. Even more\n        exciting, </span\n      ><span class="c1 c7">Grounding DINO</span\n      ><span class="c1"\n        >&nbsp;extends this capability to open-vocabulary object detection by\n        finding objects in an image based on </span\n      ><span class="c1 c7">text prompts</span\n      ><span class="c1"\n        >. These advances foreshadow a future of bioimage analysis where AI can </span\n      ><span class="c0">segment anything</span\n      ><span class="c3 c1"\n        >&nbsp;we need, even in complex experimental contexts, with minimal\n        human supervision.</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <p class="c8">\n      <span class="c1">But, how well do these tools work on </span\n      ><span class="c1 c7">biological</span\n      ><span class="c3 c1"\n        >&nbsp;images, and how are labs leveraging them? In this post, we\n        explore recent applications of SAM and DINO in biology, from microscopic\n        cell imaging to high-throughput plate assays, and how pairing them with\n        clever filtering and domain knowledge can unlock new workflows.\n        We&#39;ll also highlight case studies (academic and industry) and even\n        some of my projects using these models, giving a glimpse into the future\n        of automated bioimage analysis.</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <h3 class="c8" id="h.7jghb41kqah">\n      <span class="c9 c1">SAM: A Foundation Model for Segmentation</span>\n    </h3>\n    <p class="c8">\n      <span class="c1"\n        >Meta&#39;s AI team introduced SAM in 2023 as a general </span\n      ><span class="c1 c7">promptable segmentation</span\n      ><span class="c1"\n        >&nbsp;model that can delineate any object in an image given minimal\n        prompts (points, boxes, etc.). Trained on over a billion masks, SAM\n        boasts broad generalization. Researchers wasted no time testing SAM on\n        biomedical data. Early studies show a mix of promise and limitations:\n        SAM achieved </span\n      ><span class="c1 c7"\n        >impressive zero-shot segmentation on some medical images</span\n      ><span class="c3 c1"\n        >&nbsp;but struggled on others without fine-tuning. For example,\n        out-of-the-box SAM can outline large, high-contrast structures (like\n        organs or colonies) with minimal input, but for subtle features (faint\n        cell boundaries, noisy microscopies), its performance drops.</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <p class="c8">\n      <span class="c1 c7">Adapting SAM to specific domains</span\n      ><span class="c1"\n        >&nbsp;has been a key focus in 2024&ndash;2025. Liu </span\n      ><span class="c0">et al.</span><span class="c1">&nbsp;introduced </span\n      ><span class="c1 c7">MedSAM</span\n      ><span class="c1"\n        >, fine-tuning SAM for medical imaging to bridge the gap between natural\n        and medical domains. This year, a team led by Pape </span\n      ><span class="c0">et al.</span><span class="c1">&nbsp;released </span\n      ><span class="c1 c7">Segment Anything for Microscopy (&mu;SAM)</span\n      ><span class="c1"\n        >, which fine-tuned SAM on light and electron microscopy data. The\n        result was significantly improved segmentation quality on cell and\n        tissue images, compared to vanilla SAM. &mu;SAM even comes as a Napari\n        plugin for interactive segmentation and tracking, offering biologists a\n        unified, user-friendly tool for various microscopy modalities. These\n        efforts demonstrate that we can harness SAM&#39;s </span\n      ><span class="c1 c7">foundation model</span\n      ><span class="c3 c1"\n        >&nbsp;knowledge with modest domain-specific tuning for accurate\n        segmentation of biological structures that were previously\n        &quot;invisible&quot; to generic models.</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <p class="c8">\n      <span class="c1">It&#39;s worth noting that SAM is primarily an </span\n      ><span class="c1 c7">instance segmentation</span\n      ><span class="c1"\n        >&nbsp;model that finds object masks but doesn&#39;t label what they\n        are. In biology, we often care about </span\n      ><span class="c0">which</span\n      ><span class="c3 c1"\n        >&nbsp;cell type or colony a segment is. This need is where models like\n        DINO and Grounding DINO come in (more on that below). First, let&#39;s\n        see SAM in action in a classic microbiology problem: bacterial colony\n        counting.</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <h2 class="c8" id="h.w9pnya87hk8d">\n      <span class="c9 c1"\n        >Case Study: Bacterial Colony Segmentation on Agar Plates</span\n      >\n    </h2>\n    <p class="c8">\n      <span class="c1"\n        >Counting and analyzing bacterial colonies on agar plates is a\n        fundamental task in microbiolog that has historically required either\n        manual counting or training specialized models (e.g., U-Nets or Mask\n        R-CNNs) for segmentation. With SAM, we now have a ready-made model that\n        can </span\n      ><span class="c1 c7">segment colonies without any training</span\n      ><span class="c1"\n        >&nbsp;in specific microbiology images. Researchers have begun exploring\n        this. Ili&#263; </span\n      ><span class="c0">et al.</span\n      ><span class="c1">&nbsp;tested SAM on the </span\n      ><span class="c1 c7">AGAR dataset</span\n      ><span class="c1"\n        >&nbsp;(18,000 images of Petri dishes with various bacteria species) and\n        found that SAM could indeed detect and mask most colonies in an image\n        zero-shot. In one example, SAM produced around </span\n      ><span class="c1 c7">190 segmentation masks</span\n      ><span class="c3 c1"\n        >&nbsp;on a single petri dish image! This segmentation effectively\n        outlined each visible colony in different colors. The model even outputs\n        metadata (e.g., bounding boxes, mask area) for each object, which we can\n        export as a CSV for analysis. This showcase proved that, even without\n        training, a general model like SAM can handle dense microbial images and\n        pull out individual colony regions.</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <p class="c8">\n      <span class="c1"\n        >That said, SAM isn&#39;t perfect on these images. It may produce some\n        spurious or partial masks (e.g., fragmenting one colony into multiple\n        pieces or merging neighboring colonies into one mask). An </span\n      ><span class="c1 c7">important step is filtering SAM&#39;s output</span\n      ><span class="c1"\n        >&nbsp;to discard poor segments. In their colony analysis pipeline,\n        Sidiropoulos </span\n      ><span class="c0">et al.</span><span class="c1">&nbsp;used a </span\n      ><span class="c0">pre-trained SAM (frozen)</span\n      ><span class="c1">&nbsp;to cut out colonies and then </span\n      ><span class="c1 c7">filtered out bad masks to avoid artifacts</span\n      ><span class="c1"\n        >. What counts as a &quot;bad&quot; mask? These are often irregular\n        shapes or blurry segments that don&#39;t correspond to a single colony.\n        For instance, SAM might grab a piece of writing on the plate or a colony\n        cluster as one mask. By filtering based on properties like mask </span\n      ><span class="c1 c7">area, circularity, and solidity</span\n      ><span class="c3 c1"\n        >, one can keep only nicely rounded, reasonably sized masks (likely\n        single colonies). In the mentioned study, they explicitly removed SAM\n        masks that looked erroneous before using the rest for data\n        augmentation.</span\n      >\n    </p>\n    <p class="c13"><span class="c3 c1"></span></p>\n    <p class="c8">\n      <span class="c0"\n        >Figure: Examples of SAM&#39;s colony segmentation on an agar plate,\n        highlighting the need for filtering. Top: </span\n      ><span class="c0 c7">Good segmentations</span\n      ><span class="c0"\n        >&nbsp;&ndash; SAM masks cleanly capturing individual bacterial colonies\n        (mostly circular). Bottom: </span\n      ><span class="c0 c7">Bad segmentations</span\n      ><span class="c0 c10"\n        >&nbsp;&ndash; SAM masks that are incomplete, merged, or otherwise\n        inaccurate. By filtering out irregular masks (e.g., non-circular shapes\n        or fragments), one can automatically focus on the correct colony\n        segments.</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <p class="c8">\n      <span class="c1">Our own experience aligns with this: using </span\n      ><span class="c1 c7">SAM + circularity filtering</span\n      ><span class="c3 c1"\n        >&nbsp;on multi-well plate images, we could automatically pick out the\n        top candidate mask for each well (usually the colony or region of\n        interest) while ignoring debris or artifacts. This technique drastically\n        reduces false positives in high-throughput assays. In essence, SAM\n        provides the initial &quot;guess&quot; for every object, and a simple\n        rule-based filter (domain knowledge like &quot;colonies are round&quot;)\n        refines those guesses. It&#39;s a powerful combo of a general model with\n        a domain-specific heuristic.</span\n      >\n    </p>\n    <p class="c4"><span class="c1 c2"></span></p>\n    <h2 class="c8" id="h.4vj9v1n3kemz">\n      <span class="c9 c1">Synthetic Data Augmentation with SAM</span>\n    </h2>\n    <p class="c8">\n      <span class="c1"\n        >One remarkable case study that ties together SAM and downstream\n        detection is a recent pipeline for colony counting. Instead of training\n        a colony detector on limited real images, the authors used SAM to </span\n      ><span class="c1 c7">generate synthetic training data</span\n      ><span class="c1"\n        >. As illustrated in the figure below, the process was: take a handful\n        of real plate images &rarr; use SAM to segment every colony &rarr;\n        filter out the good colony masks &rarr; </span\n      ><span class="c1 c7">copy-paste</span\n      ><span class="c1"\n        >&nbsp;those colony cutouts onto blank agar backgrounds to make new\n        composite images (with known colony locations) &rarr; train a YOLOv8\n        detector on this synthetic dataset. They created thousands of labeled\n        synthetic images in this manner, essentially for free. The only manual\n        step was sorting out a few SAM mistakes (e.g., SAM struggled with very </span\n      ><span class="c1 c7">blurry colonies</span\n      ><span class="c3 c1">, which required manual removal).</span>\n    </p>\n    <p class="c13"><span class="c3 c1"></span></p>\n    <p class="c8">\n      <span class="c0">Figure: </span\n      ><span class="c0 c7">SAM-driven augmentation pipeline</span\n      ><span class="c10 c0"\n        >&nbsp;for bacterial colony detection. Real plate images (left) are\n        processed by SAM to extract colony masks (top center). After filtering\n        out poor masks, the high-quality cutouts are saved to a database. New\n        synthetic images are generated by pasting these colonies into empty agar\n        images (bottom center). A YOLOv8 detector is first trained on this\n        synthetic data (bottom) and then fine-tuned on a small set of real\n        images. The result (right) is an accurate colony detection model without\n        needing a large real dataset.</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <p class="c8">\n      <span class="c1"\n        >The impact of this approach was striking. With only ~100 real images\n        plus synthetic augmentation, the YOLOv8 model achieved almost the </span\n      ><span class="c1 c7"\n        >same accuracy as training on a 5&times; larger real dataset</span\n      ><span class="c1"\n        >. Specifically, 1000 real images + SAM-augmented synthetic data reached\n        a mean average precision (mAP) just a few points shy of a model trained\n        on 5241 real images. Even with as few as 50 real images, the synthetic\n        data boosted detection performance well above models trained on 50 real\n        images alone. This approach underscores how foundation models like SAM\n        can </span\n      ><span class="c0">compress the data requirement</span\n      ><span class="c3 c1"\n        >&nbsp;for new tasks by enabling good results with far less labeled data\n        by generating additional training examples. For academia and startups\n        alike, that means faster development of vision assays (less time\n        photographing and hand-labeling thousands of examples, more time getting\n        results).</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <p class="c8">\n      <span class="c1">Interestingly, while this pipeline focused on </span\n      ><span class="c1 c7">detecting all colonies</span\n      ><span class="c1"\n        >&nbsp;for hygiene monitoring (just finding any growth), the authors\n        note that they didn&#39;t yet exploit </span\n      ><span class="c1 c7">morphological features</span\n      ><span class="c1">, like colony </span><span class="c0">shape</span\n      ><span class="c1">&nbsp;or </span><span class="c0">color,</span\n      ><span class="c1"\n        >&nbsp;in their detection task. In applications like species\n        identification or assessing colony health, those features matter since\n        they suggest future work could incorporate analyses of colony </span\n      ><span class="c1 c7">pigmentation or texture</span\n      ><span class="c3 c1"\n        >. That&#39;s a perfect segue into how DINO and Grounded DINO can help\n        label or filter segments by such visual traits.</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <h2 class="c8" id="h.xovysovv97f9">\n      <span class="c1 c9"\n        >DINO: Self-Supervised Vision for Unlabeled Biological Phenomena</span\n      >\n    </h2>\n    <p class="c8">\n      <span class="c1"\n        >While SAM excels at drawing masks, it doesn&#39;t tell you what those\n        masks </span\n      ><span class="c0">are</span\n      ><span class="c1">&nbsp;or how they differ. </span\n      ><span class="c1 c7">DINO</span\n      ><span class="c1">&nbsp;(and the updated </span\n      ><span class="c1 c7">DINOv2</span\n      ><span class="c1"\n        >) are vision transformer models trained in a self-supervised manner to\n        learn rich image features. In practical terms, DINO learns to encode\n        images (or image patches) into a feature space where similar-looking\n        things cluster together (all without any human labels. In 2023, a team\n        of researchers applied DINO to microscopy images and found it had a </span\n      ><span class="c0">remarkable ability to learn cellular morphology</span\n      ><span class="c1">&nbsp;without supervision. Doron </span\n      ><span class="c0">et al.</span\n      ><span class="c1"\n        >&nbsp;showed that DINO&#39;s features of single-cell images were so\n        meaningful that simple classifiers built on them could distinguish cell\n        types and even subtle phenotypic differences nearly as well as highly\n        engineered, task-specific features. In their words, </span\n      ><span class="c0"\n        >&quot;DINO, a vision-transformer based self-supervised algorithm, has a\n        remarkable ability for learning rich representations of cellular\n        morphology&quot;</span\n      ><span class="c3 c1"\n        >. These representations were biologically faithful: different DINO\n        attention heads even aligned with different subcellular structures, like\n        nucleus versus cytoplasm, across thousands of cells.</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <p class="c8">\n      <span class="c1">For bioimage analysis, DINO opens the door to </span\n      ><span class="c1 c7">unsupervised clustering and labeling</span\n      ><span class="c1"\n        >&nbsp;of image data. Imagine you have segmented hundreds of bacterial\n        colonies with SAM. Some have </span\n      ><span class="c0">red </span\n      ><span class="c1">pigmentations, some others have a </span\n      ><span class="c0">white </span\n      ><span class="c1"\n        >pigment; meanwhile, some have smooth edges, and others are more\n        irregular. You might not have labels for these traits in advance. A\n        model like DINO can embed each colony image into a vector, such that\n        colonies with similar appearance group together in feature space.\n        Indeed, researchers have used self-supervised features to cluster cell\n        images by morphology or response to drugs, revealing meaningful\n        groupings without explicit labels. We can leverage this by taking\n        SAM&#39;s unlabeled masks and using DINO embeddings to </span\n      ><span class="c1 c7">filter or organize them by visual traits</span\n      ><span class="c3 c1"\n        >. For example, one could automatically separate pigmented vs\n        non-pigmented colonies by clustering the mask crops in DINO feature\n        space, then label those clusters post hoc (or just automatically measure\n        their color if it&#39;s as simple as hue). The key benefit is reducing\n        manual labor: instead of inspecting each mask, the model&#39;s learned\n        features do the heavy lifting.</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <p class="c8">\n      <span class="c1"\n        >In our projects, we found this helpful for things like </span\n      ><span class="c1 c7">colony pigmentation detection</span\n      ><span class="c1"\n        >. We combined SAM with DINO to identify which segmented colonies on a\n        plate were producing a specific colored pigment (a common screening\n        method in synthetic biology). SAM provided all colony regions, and\n        DINO&#39;s representation helped spot the oddballs (e.g., only a subset\n        of colonies had a dark red hue from the standard dark purple and those\n        clustered apart from the cream-colored ones in the feature space). This\n        way, we could flag pigmented colonies automatically. Recent research\n        supports this approach: self-supervised ViTs can capture even subtle\n        differences like fluorescent reporter expression or morphological\n        changes due to drugs. Essentially, foundation models can </span\n      ><span class="c1 c7">segment and characterize</span\n      ><span class="c1"\n        >&nbsp;biological samples in a two-step, label-free workflow: SAM\n        handles </span\n      ><span class="c0">&quot;where&quot;, </span\n      ><span class="c1">while DINO handles </span\n      ><span class="c0">&quot;what&#39;s different&quot;</span\n      ><span class="c3 c1">.</span>\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <h2 class="c8" id="h.ymplf5tl06ua">\n      <span class="c9 c1">Grounding DINO: Segmenting by Text Prompts</span>\n    </h2>\n    <p class="c8">\n      <span class="c1"\n        >An even more direct way to filter or label segments by their traits is\n        to use language. </span\n      ><span class="c1 c7">Grounding DINO</span\n      ><span class="c1"\n        >&nbsp;is a vision-language model that extends DINO&#39;s detection\n        abilities to work with text queries. It&#39;s a </span\n      ><span class="c1 c7">zero-shot detector</span\n      ><span class="c1"\n        >&nbsp;that can draw bounding boxes around objects described by a\n        prompt, like &quot;brown colony&quot; or &quot;clear zone&quot;, even if\n        it&#39;s never seen those exact items before. Combining it with SAM\n        (dubbed &quot;Grounded-SAM&quot;) allows an almost sci-fi workflow: </span\n      ><span class="c1 c7"\n        >type what you want to segment, and the models will find and mask\n        it</span\n      ><span class="c3 c1"\n        >. For example, in &quot;natural&quot; or &quot;commonly human&quot;\n        pictures, you could ask for &quot;cat&quot;, and Grounding DINO will\n        localize the cat while SAM segments it precisely. In the bio lab\n        setting, one might prompt &quot;white bacterial colony&quot; versus\n        &quot;red bacterial colony&quot; to have the system pick out colonies of\n        each color and mask them separately with no manual clicking\n        required.</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <p class="c8">\n      <span class="c1"\n        >While this is cutting-edge, we are starting to see it in practice. A\n        recent demo applied </span\n      ><span class="c1 c7">Grounding DINO + SAM on agricultural images</span\n      ><span class="c1"\n        >&nbsp;to detect plant seedlings by just describing them, and it worked </span\n      ><span class="c1 c7">without any fine-tuning on those images</span\n      ><span class="c1"\n        >. In our lab, we&#39;ve experimented with Grounded-SAM for tasks like\n        identifying </span\n      ><span class="c0">plate contaminants</span\n      ><span class="c1"\n        >. By providing text like &quot;fungal colony&quot; or &quot;bubble\n        artifact&quot;, the model can attempt to highlight regions that match\n        those descriptions, which SAM then segments. Of course, the accuracy\n        depends on how well the text prompt aligns with the model&#39;s learned\n        knowledge. Describing visual traits (color, shape) tends to be easier\n        (e.g., &quot;circular clear areas&quot; might help find antibiotic\n        inhibition zones); whereas very domain-specific terms might not work\n        unless the model has seen similar data in training. Still, this approach\n        of </span\n      ><span class="c1 c7">open-vocabulary segmentation</span\n      ><span class="c1"\n        >&nbsp;is extremely powerful as a concept: it means we can issue\n        high-level instructions to images and get structured outputs with a\n        drastic annotation speed up. As Piotr Skalski noted, combining Grounding\n        DINO&#39;s detection with SAM&#39;s mask generation can </span\n      ><span class="c1 c7">turbocharge image annotation</span\n      ><span class="c3 c1"\n        >, saving huge amounts of time for building new datasets.</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <p class="c8">\n      <span class="c1"\n        >For filtering SAM&#39;s many masks, one can imagine Grounding DINO\n        acting as a selector. Given SAM&#39;s pile of segments, we ask Grounding\n        DINO (or a similar vision-language model) to &quot;find the brown colony\n        among these&quot; and only keep masks overlapping with the detections.\n        This is a form of multimodal filtering: using language as a criterion\n        for vision, which is especially handy when the trait of interest is\n        easier to describe than to quantify formally. We anticipate more tools\n        and libraries building on Grounded-SAM pipelines since the\n        &quot;Grounded Segment Anything&quot; GitHub project has already\n        assembled demos that integrate Grounding DINO, SAM, and even Stable\n        Diffusion for various &quot;detect and segment anything&quot; tasks. The\n        tech is evolving rapidly, and savvy bioimage analysts can start\n        harnessing it for tasks like colony phenotype sorting, locating specific\n        structures in microscopy slides via text (e.g., &quot;mitotic\n        figure&quot;, &quot;necrotic region&quot;), and so on (all </span\n      ><span class="c1 c7">without training a new model</span\n      ><span class="c1 c3">&nbsp;for each task).</span>\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <h2 class="c8" id="h.95t92p83p4m">\n      <span class="c9 c1">Towards Automated Lab Workflows</span>\n    </h2>\n    <p class="c8">\n      <span class="c1">The ultimate promise of these models is </span\n      ><span class="c1 c7">AI-driven pipelines</span\n      ><span class="c1"\n        >&nbsp;that automatically handle routine image analysis in the lab.\n        We&#39;re already seeing prototypes of this. In high-throughput\n        screening, researchers can analyze images from multi-well plates or GPS\n        satellites on the fly: for instance, a smart microscope might quickly\n        run a YOLO detector to see if any wells have &quot;hits&quot;\n        (interesting growth or fluorescence), and if so, use SAM to segment the\n        region and measure it to automatedly decide how to proceed in real time.\n        M&uuml;ller </span\n      ><span class="c0">et al.</span\n      ><span class="c1"\n        >&nbsp;describe this scenario in exactly the following way: acquire a\n        quick low-res image, use a model to decide if something worthy is\n        present, and, if not, skip saving detailed data for that well. Such\n        decision-making could make experiments more efficient (huge data savings\n        when most wells are empty or negative). As they note, while YOLO gives a\n        rough localization (bounding box), </span\n      ><span class="c1 c7"\n        >&quot;biological objects are typically not square&quot;</span\n      ><span class="c1"\n        >, so you can feed the detection into SAM to get an accurate shape mask\n        for precise measurement. This kind of cascade (detection, then\n        segmentation) is compelling and impactful. We used a similar idea for\n        analyzing </span\n      ><span class="c1 c7">phage plaque assays</span\n      ><span class="c3 c1"\n        >&nbsp;(where viruses create clear spots in a bacterial lawn). First, a\n        detection model spots where there are clear zones on a plate, and then\n        SAM (prompted with those locations) segments the exact cleared area so\n        we can measure its size. In classical analysis of antibiotic or phage\n        inhibition zones, one would do this by thresholding or edge detection on\n        the clear region. However, SAM gives us an instant, accurate outline of\n        even irregularly shaped zones; there is no need to hand-tune intensity\n        thresholds since the model &quot;sees&quot; the absence of bacteria and\n        delineates it.</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <p class="c8">\n      <span class="c1">Incorporating these into </span\n      ><span class="c1 c7">user-friendly tools</span\n      ><span class="c1"\n        >&nbsp;is a current challenge, but progress is steady. For example, </span\n      ><span class="c1 c7">IAMSAM</span\n      ><span class="c1"\n        >, a web tool for spatial transcriptomics, uses SAM under the hood to\n        let researchers segment tissue regions by morphology and then correlate\n        those regions with gene expression data. It allows semi-automatic\n        selection of regions of interest (say, clusters of cells in a tissue),\n        which would have taken far longer to draw manually. On the industry\n        side, even major microscope software are embracing foundation models:\n        ZEISS&#39;s ZEN software recently added </span\n      ><span class="c1 c7"\n        >&quot;super-fast image annotations with SAM&quot;</span\n      ><span class="c3 c1"\n        >&nbsp;in their cloud platform. This means biologists can leverage\n        SAM&#39;s mask generation on their images through a vendor interface,\n        speeding up annotation for further analysis. Startups and biotech\n        companies are also watching closely. The ability to process an entire\n        multi-well plate of images in seconds, segment all colonies or cell\n        clusters, and flag interesting ones (maybe with an AI assistant like\n        &quot;Plato&quot; as one lab automation platform describes) is\n        incredibly appealing for scaling up experiments. The technology is\n        catching up to these needs: efficient versions of SAM (like MobileSAM\n        and EfficientSAM) are emerging to run on everyday hardware, making it\n        feasible to deploy these models in lab settings without requiring an AI\n        specialist for each use.</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <h2 class="c8" id="h.c6uyiziw7zr3">\n      <span class="c9 c1">Conclusion</span>\n    </h2>\n    <p class="c8">\n      <span class="c1"\n        >We stand at a crossroads where general AI vision models trained on\n        internet-scale data meet the specialized world of biological imaging. </span\n      ><span class="c1 c7">SAM</span\n      ><span class="c1"\n        >&nbsp;has shown that a single model can generalize to segment cells,\n        colonies, organoids (essentially, </span\n      ><span class="c0 c7">anything</span><span class="c1">)</span\n      ><span class="c1"\n        >&nbsp;with zero or minimal retraining, given the right prompts. </span\n      ><span class="c1 c7">DINO</span\n      ><span class="c1"\n        >&nbsp;and its variants demonstrate that even without labels, AI can\n        learn the subtle visual signatures of biological phenomena, from the\n        morphology of a single cell to the pigmentation of a bacterial colony.\n        With </span\n      ><span class="c1 c7">Grounded DINO</span\n      ><span class="c3 c1"\n        >, we can even speak to our images in natural language, pulling out the\n        information we care about (&quot;find the GFP-expressing cell\n        clumps&quot; or &quot;count the clear plaques&quot;).</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <p class="c8">\n      <span class="c3 c1"\n        >The future of bioimage analysis will likely be a synergy of these\n        foundation models with domain-specific knowledge. We&#39;ll see more\n        pipelines where a general model handles the heavy lifting (segmentation,\n        feature extraction, detection) and a lightweight custom layer handles\n        the specifics (filtering by shape, linking to experimental metadata,\n        etc.). For academics, this lowers the barrier to analyzing complex\n        datasets since you can bootstrap analysis with SAM+DINO and focus your\n        precious annotation time only on refining the outputs. For startups, it\n        means faster development of imaging products (no need to collect a\n        million examples of every new assay; a foundation model plus a few-shot\n        fine-tune might suffice). For recruiters in biotech/AI, it&#39;s clear\n        that familiarity with these tools is becoming a sought-after skill: they\n        enable smaller teams to achieve what only big companies with massive\n        training datasets could do a few years ago.</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <p class="c8">\n      <span class="c1"\n        >In closing, &quot;segmenting the invisible&quot; is no longer science\n        fiction in biology. We can now literally segment things we couldn&#39;t\n        even properly label before. A Petri dish teeming with microcolonies or a\n        multiplexed cell image with dozens of phenotypes, any of these can be\n        navigated and quantified with the help of SAM, DINO, and their cousins.\n        As we continue </span\n      ><span class="c1 c7">segmenting the invisible</span\n      ><span class="c1">, we make the once-inscrutable data far more </span\n      ><span class="c1 c7">visible and actionable</span\n      ><span class="c3 c1"\n        >. The hope is that this leads to quicker discoveries (finding that one\n        odd colony indicates a breakthrough mutant), more efficient workflows\n        (automating tedious image scoring), and, ultimately, a deeper\n        understanding of biological systems through the lens of cutting-edge AI.\n        The invisible world is becoming a bit more visible, one segment at a\n        time.</span\n      >\n    </p>\n    <p class="c4"><span class="c2 c1"></span></p>\n    <h2 class="c8" id="h.v8gjmp1utni2">\n      <span class="c9 c1">References and Further Reading</span>\n    </h2>\n    <ul class="c16 lst-kix_pq6l9j27g4h8-0 start">\n      <li class="c6 li-bullet-0">\n        <span class="c1">Kirillov et al. </span\n        ><span class="c0">&quot;Segment Anything.&quot;</span\n        ><span class="c3 c1">&nbsp;ICCV 2023 &ndash; Introduction of SAM.</span>\n      </li>\n      <li class="c6 li-bullet-0">\n        <span class="c1">Archit et al. </span\n        ><span class="c0"\n          >&quot;Segment Anything for Microscopy (&mu;SAM).&quot;</span\n        ><span class="c3 c1"\n          >&nbsp;Nature Methods 22, 579&ndash;591 (2025) &ndash; Fine-tuning SAM\n          for bioimages.</span\n        >\n      </li>\n      <li class="c6 li-bullet-0">\n        <span class="c1">Ilic et al. </span\n        ><span class="c0"\n          >&quot;Analysis of Microbiological Samples using SAM.&quot;</span\n        ><span class="c3 c1"\n          >&nbsp;(Conf. paper, 2023) &ndash; Applying SAM to segment colonies in\n          AGAR dataset.</span\n        >\n      </li>\n      <li class="c6 li-bullet-0">\n        <span class="c1">Kehl et al. </span\n        ><span class="c0"\n          >&quot;SAM-based Synthetic Data Augmentation for Colony\n          Detection.&quot;</span\n        ><span class="c3 c1"\n          >&nbsp;Appl. Sci. 15(3):1260 (2023) &ndash; Pipeline using SAM to\n          generate synthetic training data for colony counting.</span\n        >\n      </li>\n      <li class="c6 li-bullet-0">\n        <span class="c1">Doron et al. </span\n        ><span class="c0"\n          >&quot;Unbiased single-cell morphology with self-supervised vision\n          transformers.&quot;</span\n        ><span class="c3 c1"\n          >&nbsp;bioRxiv 2023 &ndash; DINO captures rich cell morphology\n          features without labels.</span\n        >\n      </li>\n      <li class="c6 li-bullet-0">\n        <span class="c1"\n          >Grounding DINO GitHub &ndash; Open-vocabulary object detection model\n          (2023). See also the </span\n        ><span class="c0">Grounded-SAM</span\n        ><span class="c3 c1"\n          >&nbsp;project combining text prompts, detection, and SAM\n          segmentation.</span\n        >\n      </li>\n      <li class="c6 li-bullet-0">\n        <span class="c1">Skalski, </span\n        ><span class="c0"\n          >&quot;Zero-Shot Image Annotation with Grounding DINO and\n          SAM.&quot;</span\n        ><span class="c3 c1"\n          >&nbsp;Roboflow Blog, Apr 2023 &ndash; Tutorial on using Grounded DINO\n          + SAM for faster dataset labeling.</span\n        >\n      </li>\n      <li class="c6 li-bullet-0">\n        <span class="c1">Pape et al. </span\n        ><span class="c0"\n          >&quot;IAMSAM: Image-based analysis of molecular signatures using\n          SAM.&quot;</span\n        ><span class="c3 c1"\n          >&nbsp;Genome Biology 25:290 (2024) &ndash; Tool for spatial\n          transcriptomics using SAM to segment tissue regions.</span\n        >\n      </li>\n      <li class="c6 li-bullet-0">\n        <span class="c1">Zoccoler et al. </span\n        ><span class="c0">BiAPoL Blog</span\n        ><span class="c3 c1"\n          >, Feb 2024 &ndash; Tips on combining YOLO detectors with SAM for\n          better segmentation in microscopy; introduces micro-SAM.</span\n        >\n      </li>\n      <li class="c6 li-bullet-0">\n        <span class="c3 c1"\n          >Zeiss ZEN 3.11 Release Notes (2023) &ndash; Mentions integration of\n          SAM for image annotation in microscopy software.</span\n        >\n      </li>\n    </ul>\n    <p class="c18"><span class="c3 c14"></span></p>\n  </body>\n</html>\n'},"cracking-the-minds-code":{title:"Cracking the Mind's Code | cargonriv.com Blog",description:"LCAs, neural networks, and insights from neuroscience.",html:'<!DOCTYPE html>\n<html>\n  <head>\n    <meta content="text/html; charset=UTF-8" http-equiv="content-type" />\n    <style type="text/css">\n      .lst-kix_ehzt5islygnw-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_pq6l9j27g4h8-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_ehzt5islygnw-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ehzt5islygnw-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_pq6l9j27g4h8-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_pq6l9j27g4h8-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_pq6l9j27g4h8-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_pq6l9j27g4h8-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_ehzt5islygnw-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_ehzt5islygnw-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_pq6l9j27g4h8-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_pq6l9j27g4h8-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_pq6l9j27g4h8-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ajkvb22l6j96-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ehzt5islygnw-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ehzt5islygnw-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_ikcljutuvuve-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_ehzt5islygnw-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-5 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_ajkvb22l6j96-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ehzt5islygnw-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-7 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ikcljutuvuve-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ehzt5islygnw-6 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-1 {\n        list-style-type: none;\n      }\n      .lst-kix_ajkvb22l6j96-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ajkvb22l6j96-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      ul.lst-kix_ehzt5islygnw-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-1 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-2 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-2 {\n        list-style-type: none;\n      }\n      .lst-kix_ajkvb22l6j96-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_ajkvb22l6j96-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ikcljutuvuve-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_ajkvb22l6j96-5 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      ul.lst-kix_ikcljutuvuve-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-7 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ikcljutuvuve-6 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-6 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ikcljutuvuve-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ikcljutuvuve-7 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ikcljutuvuve-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ikcljutuvuve-5 {\n        list-style-type: none;\n      }\n      .lst-kix_ehzt5islygnw-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ikcljutuvuve-2 {\n        list-style-type: none;\n      }\n      .lst-kix_ajkvb22l6j96-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_ikcljutuvuve-3 {\n        list-style-type: none;\n      }\n      .lst-kix_ehzt5islygnw-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_ikcljutuvuve-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ikcljutuvuve-1 {\n        list-style-type: none;\n      }\n      .lst-kix_ajkvb22l6j96-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_ajkvb22l6j96-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_thjubi60nqtj-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-2 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-1 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-7 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-6 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-5 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-4 {\n        list-style-type: none;\n      }\n      .lst-kix_thjubi60nqtj-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_thjubi60nqtj-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_pq6l9j27g4h8-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-5 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-6 {\n        list-style-type: none;\n      }\n      .lst-kix_thjubi60nqtj-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_pq6l9j27g4h8-0 {\n        list-style-type: none;\n      }\n      li.li-bullet-0:before {\n        margin-left: -18pt;\n        white-space: nowrap;\n        display: inline-block;\n        min-width: 18pt;\n      }\n      .lst-kix_thjubi60nqtj-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_thjubi60nqtj-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_pq6l9j27g4h8-1 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-2 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-7 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-8 {\n        list-style-type: none;\n      }\n      .lst-kix_thjubi60nqtj-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_thjubi60nqtj-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_thjubi60nqtj-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_thjubi60nqtj-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_pq6l9j27g4h8-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      ol {\n        margin: 0;\n        padding: 0;\n      }\n      table td,\n      table th {\n        padding: 0;\n      }\n      .c8 {\n        border-right-style: solid;\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        border-right-width: 0pt;\n        padding-left: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.38;\n        border-left-width: 0pt;\n        border-top-style: solid;\n        border-left-style: solid;\n        border-bottom-width: 0pt;\n        border-bottom-style: solid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n        padding-right: 0pt;\n      }\n      .c4 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.56;\n        border-top-style: solid;\n        margin-left: 51pt;\n        text-indent: -18pt;\n        border-bottom-width: 0pt;\n        border-bottom-style: solid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n        height: 11pt;\n      }\n      .c6 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        padding-left: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.56;\n        border-top-style: solid;\n        margin-left: 51pt;\n        border-bottom-width: 0pt;\n        border-bottom-style: solid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      .c13 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.56;\n        border-top-style: solid;\n        border-bottom-width: 0pt;\n        border-bottom-style: solid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n        height: 11pt;\n      }\n      .c17 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        border-bottom-width: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.38;\n        border-bottom-style: solid;\n        orphans: 2;\n        border-top-style: solid;\n        widows: 2;\n        text-align: left;\n      }\n      .c11 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        border-bottom-width: 0pt;\n        padding-bottom: 13.5pt;\n        line-height: 1.26;\n        border-bottom-style: solid;\n        orphans: 2;\n        border-top-style: solid;\n        widows: 2;\n        text-align: center;\n      }\n      .c12 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        border-bottom-width: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.56;\n        border-bottom-style: solid;\n        orphans: 2;\n        border-top-style: solid;\n        widows: 2;\n        text-align: center;\n      }\n      .c18 {\n        padding-top: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.15;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n        height: 11pt;\n      }\n      .c15 {\n        font-weight: 400;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 26pt;\n        font-family: "Arial";\n        font-style: normal;\n      }\n      .c9 {\n        font-weight: 400;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 17pt;\n        font-family: "Arial";\n        font-style: normal;\n      }\n      .c3 {\n        font-weight: 400;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 11pt;\n        font-family: "Arial";\n        font-style: normal;\n      }\n      .c2 {\n        font-weight: 400;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 12pt;\n        font-family: "Arial";\n        font-style: normal;\n      }\n      .c10 {\n        font-weight: 400;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 11pt;\n        font-family: "Arial";\n      }\n      .c5 {\n        background-color: #ffffff;\n        max-width: 468pt;\n        padding: 72pt 72pt 72pt 72pt;\n      }\n      .c0 {\n        color: #0e101a;\n        font-style: italic;\n      }\n      .c16 {\n        padding: 0;\n        margin: 0;\n      }\n      .c1 {\n        color: #0e101a;\n      }\n      .c7 {\n        font-weight: 700;\n      }\n      .c14 {\n        color: #000000;\n      }\n      .title {\n        padding-top: 0pt;\n        color: #000000;\n        font-size: 26pt;\n        padding-bottom: 3pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      .subtitle {\n        padding-top: 0pt;\n        color: #666666;\n        font-size: 15pt;\n        padding-bottom: 16pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      li {\n        color: #000000;\n        font-size: 11pt;\n        font-family: "Arial";\n      }\n      p {\n        margin: 0;\n        color: #000000;\n        font-size: 11pt;\n        font-family: "Arial";\n      }\n      h1 {\n        padding-top: 20pt;\n        color: #000000;\n        font-size: 20pt;\n        padding-bottom: 6pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h2 {\n        padding-top: 18pt;\n        color: #ffffff;\n        font-size: 16pt;\n        padding-bottom: 6pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h3 {\n        padding-top: 16pt;\n        color: #434343;\n        font-size: 14pt;\n        padding-bottom: 4pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h4 {\n        padding-top: 14pt;\n        color: #666666;\n        font-size: 12pt;\n        padding-bottom: 4pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h5 {\n        padding-top: 12pt;\n        color: #666666;\n        font-size: 11pt;\n        padding-bottom: 4pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h6 {\n        padding-top: 12pt;\n        color: #666666;\n        font-size: 11pt;\n        padding-bottom: 4pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        font-style: italic;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n    </style>\n    <style>\n      body.c5 {\n        background-color: #0e101a;\n      }\n      .title,\n      .subtitle,\n      h1,\n      h2,\n      h3,\n      h4,\n      h5,\n      h6,\n      p,\n      li,\n      span {\n        color: #ffffff !important;\n      }\n      .title {\n        font-size: 36pt !important;\n        text-align: center !important;\n      }\n      .subtitle {\n        font-size: 24pt !important;\n        text-align: center !important;\n      }\n      h1 {\n        font-size: 30pt !important;\n      }\n      h2 {\n        font-size: 24pt !important;\n      }\n      h3 {\n        font-size: 20pt !important;\n      }\n      h4 {\n        font-size: 18pt !important;\n      }\n      h5 {\n        font-size: 16pt !important;\n      }\n      h6 {\n        font-size: 14pt !important;\n      }\n      p {\n        margin-bottom: 1em;\n      }\n    </style>\n  </head>\n  <body class="c16 doc-content">\n    <h1 class="c17" id="h.2hs3cuxuwrzn">\n      <span class="c5 c13">Cracking the Mind&#39;s Code: </span>\n    </h1>\n    <h2 class="c11" id="h.hi14f5y5q64x">\n      <span class="c7 c5"\n        >How Neural Networks and Mice Brains Unveil Our Inner Workings</span\n      >\n    </h2>\n    <p class="c9"><span class="c2 c5">Hey there, fellow readers!</span></p>\n    <p class="c0">\n      <span class="c2 c5"\n        >Today, we&rsquo;re diving deep (like &quot;Alice in\n        Wonderland&quot;-rabbit-hole deep) into the fascinating world of neural\n        networks. But we&rsquo;re adding a twist. Ever heard of Locally\n        Competitive Algorithms (LCAs) with accumulator neurons? No? Buckle\n        up!</span\n      >\n    </p>\n    <h3 class="c6" id="h.p2inkn73z0to">\n      <span class="c7 c5">The Journey Begins: What Are LCAs?</span>\n    </h3>\n    <p class="c0">\n      <span class="c2"\n        >Before we get into the nitty-gritty, let&#39;s unpack what LCAs are.\n        Simply put, LCAs allow for sparse coding. If you&#39;re intrigued by\n        this concept, you&#39;re not alone. This field has seen significant\n        advancements, thanks in part to Dr. Rozell&#39;s seminal paper,\n        &quot;Sparse Coding via Thresholding and Local Competition in Neural\n        Circuits&quot; </span\n      ><span class="c1"\n        ><a\n          class="c4"\n          href="https://www.google.com/url?q=https://pubmed.ncbi.nlm.nih.gov/18439138/&amp;sa=D&amp;source=editors&amp;ust=1755852787307410&amp;usg=AOvVaw2L5GfQCGwIgqXR_dVw4Hz0"\n          >1</a\n        ></span\n      ><span class="c2 c5"\n        >. Think of LCAs as the &#39;Marie Kondos&#39; of the neural network\n        world: they keep what sparks joy (i.e., essential features) and toss the\n        rest. LCAs have been around the block, but what&rsquo;s really rocking\n        the boat is integrating them with accumulator neurons.</span\n      >\n    </p>\n    <h3 class="c6" id="h.xpug5iqfl3bt">\n      <span class="c7 c5">The Special Sauce: Accumulator Neurons</span>\n    </h3>\n    <p class="c0">\n      <span class="c2 c5"\n        >Accumulator neurons act like tiny accountants, meticulously keeping\n        track of data. When paired with LCAs, they become the dynamic duo we\n        never knew we needed. These neurons store and process information in a\n        way that&rsquo;s more aligned with biological neural networks, even if\n        they don&rsquo;t leak like biological ones. They make LCAs not just good\n        but great.</span\n      >\n    </p>\n    <h3 class="c6" id="h.o7epep2petep">\n      <span class="c5 c7">The Real Deal: Mice &amp; Vision</span>\n    </h3>\n    <p class="c0">\n      <span class="c2"\n        >Here comes the fun part: applying this techie marvel to actual, living,\n        breathing mice. One dictionary in our model learns one-dimensional\n        features from the fluorescent traces observed in the V1 layer of the\n        mice&rsquo;s visual cortex. The other dictionary? It&rsquo;s all about\n        those 3D features from what the traces&#39; recordings capture as\n        simultaneous stimuli, like a movie or a GIF that mice may observe while\n        capturing the fluorescent traces&#39; data. This particular model is\n        inspired by the research paper &quot;Dictionary Learning with\n        Accumulator Neurons&quot; </span\n      ><span class="c1"\n        ><a\n          class="c4"\n          href="https://www.google.com/url?q=https://arxiv.org/pdf/2205.15386.pdf&amp;sa=D&amp;source=editors&amp;ust=1755852787309446&amp;usg=AOvVaw21nUNYayBKELiqDZHi8-Q0"\n          >2</a\n        ></span\n      ><span class="c2 c5">.</span>\n    </p>\n    <h3 class="c12" id="h.jqiv01mpildd">\n      <span class="c3">Why Two Dictionaries?</span>\n    </h3>\n    <p class="c0">\n      <span class="c2 c5"\n        >Why stop at one dictionary when you can have two, each serving a\n        specific purpose? The first dictionary focuses on the 1D features,\n        making it easier to interpret and analyze the mice&rsquo;s visual\n        cortex. The second dictionary dives into 2D or 3D features, allowing us\n        to reconstruct an arbitrary amount of artificial neurons&#39; activities\n        and the corresponding image or video input fidelity. It&rsquo;s like\n        having two superheroes in one comic book!</span\n      >\n    </p>\n    <h3 class="c12" id="h.kyhad02effqb">\n      <span class="c3">The Future is Bright</span>\n    </h3>\n    <p class="c0">\n      <span class="c2 c5"\n        >The future work is nothing short of exciting. Imagine linking two\n        dictionaries to the same input! This paves the way for creating more\n        accurate and flexible models that can adapt to various types of neural\n        data. We&rsquo;re talking next-level neuroengineering, folks.</span\n      >\n    </p>\n    <h3 class="c12" id="h.vy5fywo59plo"><span class="c3">Wrapping Up</span></h3>\n    <p class="c0">\n      <span class="c2 c5"\n        >So there you have it! LCAs, accumulator neurons, and a couple of mice\n        later, we&#39;ve got ourselves a model that&rsquo;s a game-changer in\n        the realm of neural networks and neuroengineering. Stay curious, keep\n        exploring, and who knows what we&#39;ll unravel next!</span\n      >\n    </p>\n    \n    <h3 class="c12" id="h.9uq7cdobct50"><span class="c3">Footnotes</span></h3>\n    <ul class="c15 lst-kix_wuqxkk5qzk1p-0 start">\n      <li class="c0 c18 li-bullet-0">\n        <span class="c2"\n          >Rozell, C. J., Johnson, D. H., Baraniuk, R. G., &amp; Olshausen, B.\n          A. (2008). Sparse Coding via Thresholding and Local Competition in\n          Neural Circuits. </span\n        ><span class="c1"\n          ><a\n            class="c4"\n            href="https://www.google.com/url?q=https://canvas.harvard.edu/courses/1318/files/366092/download?verifier%3DXddkaSkGGlQpGYf1LjFBq10gWV7InBtk6ERj898d%26wrap%3D1&amp;sa=D&amp;source=editors&amp;ust=1755852787311700&amp;usg=AOvVaw0KDWI_cTzxSHWUYobUsTNr"\n            >Download Paper PDF &#8617;</a\n          ></span\n        >\n      </li>\n      <li class="c0 c14 li-bullet-0">\n        <span class="c2"\n          >Parpart, G., Gonzalez, C., Watkins, Y., Kenyon, G. T., Stewart, T.\n          C., Kim, E., Rego, J., &amp; Nesbit, S. C., O&#39;Brien, A. (2022).\n          Dictionary Learning with Accumulator Neurons. </span\n        ><span class="c1"\n          ><a\n            class="c4"\n            href="https://www.google.com/url?q=https://arxiv.org/pdf/2205.15386.pdf&amp;sa=D&amp;source=editors&amp;ust=1755852787312183&amp;usg=AOvVaw1jaBZiSj0JG_zQ7w8OKsZ2"\n            >Link to Paper PDF&#8617;</a\n          ></span\n        >\n      </li>\n    </ul>\n    <p class="c8"><span class="c5 c19"></span></p>\n  </body>\n</html>\n'},"data-science-tutorials":{title:"Data Science Tools & Tutorials | cargonriv.com Blog",description:"Curated free resources for Python and ML fundamentals.",html:'<!DOCTYPE html>\n<html>\n  <head>\n    <meta content="text/html; charset=UTF-8" http-equiv="content-type" />\n    <style type="text/css">\n      @import url(https://themes.googleusercontent.com/fonts/css?kit=RFda8w1V0eDZheqfcyQ4EGb3DKsRMD34dqg1gT8Z-p7OfsfM6rvuuu7h1pY3r_-A);\n      .lst-kix_ehzt5islygnw-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_ehzt5islygnw-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ehzt5islygnw-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_yq82ov67ns44-7 {\n        list-style-type: none;\n      }\n      ul.lst-kix_yq82ov67ns44-6 {\n        list-style-type: none;\n      }\n      .lst-kix_ehzt5islygnw-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      ul.lst-kix_yq82ov67ns44-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_yq82ov67ns44-3 {\n        list-style-type: none;\n      }\n      .lst-kix_ehzt5islygnw-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_yq82ov67ns44-2 {\n        list-style-type: none;\n      }\n      ul.lst-kix_yq82ov67ns44-5 {\n        list-style-type: none;\n      }\n      ul.lst-kix_yq82ov67ns44-4 {\n        list-style-type: none;\n      }\n      .lst-kix_5w27fsssi78y-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_6i1svtfte0kf-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_6i1svtfte0kf-5 {\n        list-style-type: none;\n      }\n      .lst-kix_5w27fsssi78y-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_6i1svtfte0kf-2 {\n        list-style-type: none;\n      }\n      ul.lst-kix_6i1svtfte0kf-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_6i1svtfte0kf-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_6i1svtfte0kf-6 {\n        list-style-type: none;\n      }\n      ul.lst-kix_6i1svtfte0kf-7 {\n        list-style-type: none;\n      }\n      .lst-kix_5w27fsssi78y-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ehzt5islygnw-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_5w27fsssi78y-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_ehzt5islygnw-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_6i1svtfte0kf-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_6i1svtfte0kf-1 {\n        list-style-type: none;\n      }\n      .lst-kix_5w27fsssi78y-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_6i1svtfte0kf-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_5w27fsssi78y-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_6i1svtfte0kf-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_ikcljutuvuve-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_6i1svtfte0kf-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_5w27fsssi78y-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_ikcljutuvuve-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_6i1svtfte0kf-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_5w27fsssi78y-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_ikcljutuvuve-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ikcljutuvuve-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_6i1svtfte0kf-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_6i1svtfte0kf-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_5w27fsssi78y-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_ajkvb22l6j96-1 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-3 {\n        list-style-type: none;\n      }\n      .lst-kix_6i1svtfte0kf-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_ajkvb22l6j96-2 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_yq82ov67ns44-1 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-5 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      ul.lst-kix_ikcljutuvuve-8 {\n        list-style-type: none;\n      }\n      .lst-kix_6i1svtfte0kf-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_yq82ov67ns44-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-7 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ikcljutuvuve-6 {\n        list-style-type: none;\n      }\n      .lst-kix_6i1svtfte0kf-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_ajkvb22l6j96-6 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ikcljutuvuve-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ikcljutuvuve-7 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ikcljutuvuve-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ikcljutuvuve-5 {\n        list-style-type: none;\n      }\n      .lst-kix_ehzt5islygnw-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ikcljutuvuve-2 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ikcljutuvuve-3 {\n        list-style-type: none;\n      }\n      .lst-kix_ehzt5islygnw-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_ikcljutuvuve-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ikcljutuvuve-1 {\n        list-style-type: none;\n      }\n      .lst-kix_xfm1dkwgdsb2-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_xfm1dkwgdsb2-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_xfm1dkwgdsb2-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_xfm1dkwgdsb2-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_xfm1dkwgdsb2-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_xfm1dkwgdsb2-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_wuqxkk5qzk1p-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_xfm1dkwgdsb2-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_wuqxkk5qzk1p-6 {\n        list-style-type: none;\n      }\n      ul.lst-kix_xfm1dkwgdsb2-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_wuqxkk5qzk1p-7 {\n        list-style-type: none;\n      }\n      ul.lst-kix_xfm1dkwgdsb2-2 {\n        list-style-type: none;\n      }\n      ul.lst-kix_wuqxkk5qzk1p-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_xfm1dkwgdsb2-1 {\n        list-style-type: none;\n      }\n      ul.lst-kix_wuqxkk5qzk1p-5 {\n        list-style-type: none;\n      }\n      ul.lst-kix_xfm1dkwgdsb2-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_wuqxkk5qzk1p-2 {\n        list-style-type: none;\n      }\n      ul.lst-kix_xfm1dkwgdsb2-7 {\n        list-style-type: none;\n      }\n      ul.lst-kix_wuqxkk5qzk1p-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_xfm1dkwgdsb2-6 {\n        list-style-type: none;\n      }\n      ul.lst-kix_wuqxkk5qzk1p-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_xfm1dkwgdsb2-5 {\n        list-style-type: none;\n      }\n      ul.lst-kix_wuqxkk5qzk1p-1 {\n        list-style-type: none;\n      }\n      .lst-kix_pq6l9j27g4h8-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_wuqxkk5qzk1p-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_pq6l9j27g4h8-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_wuqxkk5qzk1p-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_pq6l9j27g4h8-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_pq6l9j27g4h8-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_pq6l9j27g4h8-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_wuqxkk5qzk1p-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_wuqxkk5qzk1p-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_yq82ov67ns44-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_wuqxkk5qzk1p-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_pq6l9j27g4h8-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_wuqxkk5qzk1p-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_pq6l9j27g4h8-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_yq82ov67ns44-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_wuqxkk5qzk1p-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_pq6l9j27g4h8-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ajkvb22l6j96-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_wuqxkk5qzk1p-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ehzt5islygnw-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-5 {\n        list-style-type: none;\n      }\n      .lst-kix_ajkvb22l6j96-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ehzt5islygnw-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-7 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-6 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-1 {\n        list-style-type: none;\n      }\n      .lst-kix_ajkvb22l6j96-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ajkvb22l6j96-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      ul.lst-kix_ehzt5islygnw-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-2 {\n        list-style-type: none;\n      }\n      .lst-kix_ajkvb22l6j96-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_ajkvb22l6j96-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_xfm1dkwgdsb2-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_ajkvb22l6j96-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_xfm1dkwgdsb2-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_ajkvb22l6j96-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_ajkvb22l6j96-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_xfm1dkwgdsb2-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_xfm1dkwgdsb2-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_thjubi60nqtj-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-2 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-1 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-7 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-6 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-5 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-4 {\n        list-style-type: none;\n      }\n      .lst-kix_thjubi60nqtj-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_5w27fsssi78y-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_5w27fsssi78y-7 {\n        list-style-type: none;\n      }\n      ul.lst-kix_5w27fsssi78y-6 {\n        list-style-type: none;\n      }\n      .lst-kix_yq82ov67ns44-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_yq82ov67ns44-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_thjubi60nqtj-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_5w27fsssi78y-1 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_5w27fsssi78y-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-5 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-6 {\n        list-style-type: none;\n      }\n      .lst-kix_thjubi60nqtj-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_5w27fsssi78y-5 {\n        list-style-type: none;\n      }\n      ul.lst-kix_5w27fsssi78y-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-0 {\n        list-style-type: none;\n      }\n      li.li-bullet-0:before {\n        margin-left: -18pt;\n        white-space: nowrap;\n        display: inline-block;\n        min-width: 18pt;\n      }\n      .lst-kix_thjubi60nqtj-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_thjubi60nqtj-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_5w27fsssi78y-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-1 {\n        list-style-type: none;\n      }\n      ul.lst-kix_5w27fsssi78y-2 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-2 {\n        list-style-type: none;\n      }\n      .lst-kix_yq82ov67ns44-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_yq82ov67ns44-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      ul.lst-kix_pq6l9j27g4h8-7 {\n        list-style-type: none;\n      }\n      .lst-kix_yq82ov67ns44-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_pq6l9j27g4h8-8 {\n        list-style-type: none;\n      }\n      .lst-kix_thjubi60nqtj-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_yq82ov67ns44-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_thjubi60nqtj-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_thjubi60nqtj-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_yq82ov67ns44-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_thjubi60nqtj-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_pq6l9j27g4h8-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_wuqxkk5qzk1p-0 > li:before {\n        content: "\\0025cf   ";\n      }\n      ol {\n        margin: 0;\n        padding: 0;\n      }\n      table td,\n      table th {\n        padding: 0;\n      }\n      .c6 {\n        border-right-style: solid;\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        border-right-width: 0pt;\n        padding-left: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.56;\n        border-left-width: 0pt;\n        border-top-style: solid;\n        margin-left: 51pt;\n        border-left-style: solid;\n        border-bottom-width: 0pt;\n        border-bottom-style: solid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n        padding-right: 0pt;\n      }\n      .c7 {\n        border-right-style: solid;\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        border-right-width: 0pt;\n        padding-left: 0pt;\n        padding-bottom: 13.5pt;\n        line-height: 1.26;\n        border-left-width: 0pt;\n        border-top-style: solid;\n        border-left-style: solid;\n        border-bottom-width: 0pt;\n        border-bottom-style: solid;\n        orphans: 2;\n        widows: 2;\n        text-align: center;\n        padding-right: 0pt;\n      }\n      .c0 {\n        border-right-style: solid;\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        border-right-width: 0pt;\n        padding-left: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.38;\n        border-left-width: 0pt;\n        border-top-style: solid;\n        border-left-style: solid;\n        border-bottom-width: 0pt;\n        border-bottom-style: solid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n        padding-right: 0pt;\n      }\n      .c3 {\n        color: #000000;\n        font-weight: 400;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 12pt;\n        font-family: "Merriweather";\n        font-style: normal;\n      }\n      .c1 {\n        -webkit-text-decoration-skip: none;\n        color: #1155cc;\n        font-weight: 400;\n        text-decoration: underline;\n        text-decoration-skip-ink: none;\n        font-size: 12pt;\n        font-family: "Merriweather";\n      }\n      .c5 {\n        color: #212121;\n        font-weight: 400;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 12pt;\n        font-family: "Merriweather";\n        font-style: normal;\n      }\n      .c2 {\n        color: #000000;\n        font-weight: 700;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 45pt;\n        font-family: "Merriweather";\n        font-style: italic;\n      }\n      .c13 {\n        color: #000000;\n        font-weight: 400;\n        text-decoration: none;\n        font-size: 11pt;\n        font-family: "Arial";\n      }\n      .c10 {\n        color: #000000;\n        font-weight: 400;\n        text-decoration: none;\n        font-size: 12pt;\n        font-family: "Arial";\n      }\n      .c11 {\n        background-color: #ffffff;\n        max-width: 468pt;\n        padding: 72pt 72pt 72pt 72pt;\n      }\n      .c4 {\n        color: inherit;\n        text-decoration: inherit;\n      }\n      .c8 {\n        vertical-align: baseline;\n        font-style: normal;\n      }\n      .c12 {\n        padding: 0;\n        margin: 0;\n      }\n      .c9 {\n        height: 11pt;\n      }\n      .title {\n        padding-top: 0pt;\n        color: #000000;\n        font-size: 26pt;\n        padding-bottom: 3pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      .subtitle {\n        padding-top: 0pt;\n        color: #666666;\n        font-size: 15pt;\n        padding-bottom: 16pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      li {\n        color: #000000;\n        font-size: 11pt;\n        font-family: "Arial";\n      }\n      p {\n        margin: 0;\n        color: #000000;\n        font-size: 11pt;\n        font-family: "Arial";\n      }\n      h1 {\n        padding-top: 20pt;\n        color: #000000;\n        font-size: 20pt;\n        padding-bottom: 6pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h2 {\n        padding-top: 18pt;\n        color: #000000;\n        font-size: 16pt;\n        padding-bottom: 6pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h3 {\n        padding-top: 16pt;\n        color: #434343;\n        font-size: 14pt;\n        padding-bottom: 4pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h4 {\n        padding-top: 14pt;\n        color: #666666;\n        font-size: 12pt;\n        padding-bottom: 4pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h5 {\n        padding-top: 12pt;\n        color: #666666;\n        font-size: 11pt;\n        padding-bottom: 4pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h6 {\n        padding-top: 12pt;\n        color: #666666;\n        font-size: 11pt;\n        padding-bottom: 4pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        font-style: italic;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n    </style>\n  </head>\n  <body class="c11 doc-content">\n    <h1 class="c7" id="h.raaslmz7btrc">\n      <span class="c2">Data Science Tools &amp; Tutorials</span>\n    </h1>\n    <p class="c0">\n      <span class="c3"\n        >Here are a few tutorials, FOR FREE, about the Python programming\n        language that hint towards data science basics:</span\n      >\n    </p>\n    <ul class="c12 lst-kix_yq82ov67ns44-0 start">\n      <li class="c6 li-bullet-0">\n        <span class="c1"\n          ><a\n            class="c4"\n            href="https://www.google.com/url?q=https://www.coursera.org/professional-certificates/ai-engineer&amp;sa=D&amp;source=editors&amp;ust=1755856631517660&amp;usg=AOvVaw0sMF744gZ36pSFI2KdRZ-r"\n            >AI Engineering Certificate (from Coursera)</a\n          ></span\n        >\n      </li>\n      <li class="c6 li-bullet-0">\n        <span class="c1"\n          ><a\n            class="c4"\n            href="https://www.google.com/url?q=https://ehmatthes.github.io/pcc/&amp;sa=D&amp;source=editors&amp;ust=1755856631518198&amp;usg=AOvVaw0NjbCSoqD4Hsbe9HAU5cGo"\n            >Python Crash Course (aka &quot;Python For Dummies&quot; book)</a\n          ></span\n        >\n      </li>\n      <li class="c6 li-bullet-0">\n        <span class="c1"\n          ><a\n            class="c4"\n            href="https://www.google.com/url?q=https://www.youtube.com/@coreyms/playlists&amp;sa=D&amp;source=editors&amp;ust=1755856631518738&amp;usg=AOvVaw1m7WjgChuI17qrZL13Nxns"\n            >Corey Schafer&#39;s YouTube Playlists (mainly) about Python</a\n          ></span\n        >\n      </li>\n      <li class="c6 li-bullet-0">\n        <span class="c1"\n          ><a\n            class="c4"\n            href="https://www.google.com/url?q=https://automatetheboringstuff.com/&amp;sa=D&amp;source=editors&amp;ust=1755856631519259&amp;usg=AOvVaw2VK6FnA1Uqw5j_F2efKnOm"\n            >Automate the Boring Stuff with Python (another &quot;Python For\n            Dummies&quot;)</a\n          ></span\n        >\n      </li>\n      <li class="c6 li-bullet-0">\n        <span class="c1"\n          ><a\n            class="c4"\n            href="https://www.google.com/url?q=https://www.coursera.org/specializations/machine-learning-introduction&amp;sa=D&amp;source=editors&amp;ust=1755856631520082&amp;usg=AOvVaw2UvlbFrH1SGUFD9uY-qEsn"\n            >Andrew Ng&#39;s Machine Learning Specialization (Stanford-led from\n            Coursera)</a\n          ></span\n        >\n      </li>\n    </ul>\n    <p class="c0 c9"><span class="c8 c10"></span></p>\n    <p class="c0 c9"><span class="c10 c8"></span></p>\n    <p class="c0">\n      <span class="c5"\n        >Below are a two paid alternatives (no more than $20.00 each course)\n        that cover wider aspects like web-development demonstrations and future\n        interpretations or visuals about the results from the Pythonic\n        evaluations of your dataset and its domain using data science\n        tools:</span\n      >\n    </p>\n    <ul class="c12 lst-kix_xfm1dkwgdsb2-0 start">\n      <li class="c6 li-bullet-0">\n        <span class="c1"\n          ><a\n            class="c4"\n            href="https://www.google.com/url?q=https://www.udemy.com/course/100-days-of-code/?utm_source%3Dadwords%26utm_medium%3Dudemyads%26utm_campaign%3DPython_v.PROF_la.EN_cc.US_ti.7380%26utm_content%3Ddeal4584%26utm_term%3D_._ag_78513466559_._ad_532070164200_._kw__._de_c_._dm__._pl__._ti_dsa-774930046209_._li_9007268_._pd__._%26matchtype%3D%26gclid%3DCjwKCAiAiKuOBhBQEiwAId_sKxTOOcgrB7KkvWWOES9j9uHuypXbAy7KEIRt-u2OH9XttvynXAiOZhoCyvEQAvD_BwE&amp;sa=D&amp;source=editors&amp;ust=1755856631523672&amp;usg=AOvVaw2kemxw3tY8J1OiLzk11Cql"\n            >100 Days of Code by Dr. Angela Yu: The Complete Python Pro Bootcamp\n            (from Udemy, for beginning students)</a\n          ></span\n        >\n      </li>\n      <li class="c6 li-bullet-0">\n        <span class="c1"\n          ><a\n            class="c4"\n            href="https://www.google.com/url?q=https://www.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/?utm_source%3Dadwords%26utm_medium%3Dudemyads%26utm_campaign%3DPython_v.PROF_la.EN_cc.US_ti.7380%26utm_content%3Ddeal4584%26utm_term%3D_._ag_78513466559_._ad_532070164200_._kw__._de_c_._dm__._pl__._ti_dsa-774930046209_._li_9007268_._pd__._%26matchtype%3D%26gclid%3DCjwKCAiAzrWOBhBjEiwAq85QZ6wKCUm200fEiKncAczmqqU0QYbuGFyBqSXFoTamIFtv_3Z63hWM6RoCE_MQAvD_BwE&amp;sa=D&amp;source=editors&amp;ust=1755856631526737&amp;usg=AOvVaw2qYSLlZJqSIrxtmpM1D-8-"\n            >Python for Data Science and Machine Learning Bootcamp (from Udemy,\n            for intermediate students)</a\n          ></span\n        >\n      </li>\n    </ul>\n  </body>\n</html>\n'},"navigating-neurons":{title:"Navigating our Neurons Highways | cargonriv.com Blog",description:"A friendly journey into data science and neuroengineering.",html:'<!DOCTYPE html>\n<html>\n  <head>\n    <meta content="text/html; charset=UTF-8" http-equiv="content-type" />\n    <style type="text/css">\n      .lst-kix_ehzt5islygnw-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_pq6l9j27g4h8-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_ehzt5islygnw-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ehzt5islygnw-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_pq6l9j27g4h8-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_pq6l9j27g4h8-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_pq6l9j27g4h8-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_pq6l9j27g4h8-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_ehzt5islygnw-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_ehzt5islygnw-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_pq6l9j27g4h8-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_pq6l9j27g4h8-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_pq6l9j27g4h8-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ajkvb22l6j96-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ehzt5islygnw-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ehzt5islygnw-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_ikcljutuvuve-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_ehzt5islygnw-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-5 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_ajkvb22l6j96-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ehzt5islygnw-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-7 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ikcljutuvuve-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ehzt5islygnw-6 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-1 {\n        list-style-type: none;\n      }\n      .lst-kix_ajkvb22l6j96-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ajkvb22l6j96-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      ul.lst-kix_ehzt5islygnw-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-1 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ehzt5islygnw-2 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-2 {\n        list-style-type: none;\n      }\n      .lst-kix_ajkvb22l6j96-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_ajkvb22l6j96-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ikcljutuvuve-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_ajkvb22l6j96-5 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      ul.lst-kix_ikcljutuvuve-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-7 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ikcljutuvuve-6 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-6 {\n        list-style-type: none;\n      }\n      .lst-kix_ikcljutuvuve-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_ikcljutuvuve-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ikcljutuvuve-7 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ikcljutuvuve-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ajkvb22l6j96-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ikcljutuvuve-5 {\n        list-style-type: none;\n      }\n      .lst-kix_ehzt5islygnw-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_ikcljutuvuve-2 {\n        list-style-type: none;\n      }\n      .lst-kix_ajkvb22l6j96-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_ikcljutuvuve-3 {\n        list-style-type: none;\n      }\n      .lst-kix_ehzt5islygnw-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_ikcljutuvuve-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_ikcljutuvuve-1 {\n        list-style-type: none;\n      }\n      .lst-kix_ajkvb22l6j96-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_ajkvb22l6j96-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_thjubi60nqtj-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-2 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-1 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-0 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-7 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-6 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-5 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-4 {\n        list-style-type: none;\n      }\n      .lst-kix_thjubi60nqtj-1 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_thjubi60nqtj-2 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_pq6l9j27g4h8-3 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-4 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-5 {\n        list-style-type: none;\n      }\n      ul.lst-kix_thjubi60nqtj-8 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-6 {\n        list-style-type: none;\n      }\n      .lst-kix_thjubi60nqtj-7 > li:before {\n        content: "\\0025cb   ";\n      }\n      ul.lst-kix_pq6l9j27g4h8-0 {\n        list-style-type: none;\n      }\n      li.li-bullet-0:before {\n        margin-left: -18pt;\n        white-space: nowrap;\n        display: inline-block;\n        min-width: 18pt;\n      }\n      .lst-kix_thjubi60nqtj-0 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_thjubi60nqtj-8 > li:before {\n        content: "\\0025a0   ";\n      }\n      ul.lst-kix_pq6l9j27g4h8-1 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-2 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-7 {\n        list-style-type: none;\n      }\n      ul.lst-kix_pq6l9j27g4h8-8 {\n        list-style-type: none;\n      }\n      .lst-kix_thjubi60nqtj-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_thjubi60nqtj-5 > li:before {\n        content: "\\0025a0   ";\n      }\n      .lst-kix_thjubi60nqtj-3 > li:before {\n        content: "\\0025cf   ";\n      }\n      .lst-kix_thjubi60nqtj-4 > li:before {\n        content: "\\0025cb   ";\n      }\n      .lst-kix_pq6l9j27g4h8-6 > li:before {\n        content: "\\0025cf   ";\n      }\n      ol {\n        margin: 0;\n        padding: 0;\n      }\n      table td,\n      table th {\n        padding: 0;\n      }\n      .c8 {\n        border-right-style: solid;\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        border-right-width: 0pt;\n        padding-left: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.38;\n        border-left-width: 0pt;\n        border-top-style: solid;\n        border-left-style: solid;\n        border-bottom-width: 0pt;\n        border-bottom-style: solid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n        padding-right: 0pt;\n      }\n      .c4 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.56;\n        border-top-style: solid;\n        margin-left: 51pt;\n        text-indent: -18pt;\n        border-bottom-width: 0pt;\n        border-bottom-style: solid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n        height: 11pt;\n      }\n      .c6 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        padding-left: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.56;\n        border-top-style: solid;\n        margin-left: 51pt;\n        border-bottom-width: 0pt;\n        border-bottom-style: solid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      .c13 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.56;\n        border-top-style: solid;\n        border-bottom-width: 0pt;\n        border-bottom-style: solid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n        height: 11pt;\n      }\n      .c17 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        border-bottom-width: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.38;\n        border-bottom-style: solid;\n        orphans: 2;\n        border-top-style: solid;\n        widows: 2;\n        text-align: left;\n      }\n      .c11 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        border-bottom-width: 0pt;\n        padding-bottom: 13.5pt;\n        line-height: 1.26;\n        border-bottom-style: solid;\n        orphans: 2;\n        border-top-style: solid;\n        widows: 2;\n        text-align: center;\n      }\n      .c12 {\n        padding-top: 0pt;\n        border-top-width: 0pt;\n        border-bottom-width: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.56;\n        border-bottom-style: solid;\n        orphans: 2;\n        border-top-style: solid;\n        widows: 2;\n        text-align: center;\n      }\n      .c18 {\n        padding-top: 0pt;\n        padding-bottom: 0pt;\n        line-height: 1.15;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n        height: 11pt;\n      }\n      .c15 {\n        font-weight: 400;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 26pt;\n        font-family: "Arial";\n        font-style: normal;\n      }\n      .c9 {\n        font-weight: 400;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 17pt;\n        font-family: "Arial";\n        font-style: normal;\n      }\n      .c3 {\n        font-weight: 400;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 11pt;\n        font-family: "Arial";\n        font-style: normal;\n      }\n      .c2 {\n        font-weight: 400;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 12pt;\n        font-family: "Arial";\n        font-style: normal;\n      }\n      .c10 {\n        font-weight: 400;\n        text-decoration: none;\n        vertical-align: baseline;\n        font-size: 11pt;\n        font-family: "Arial";\n      }\n      .c5 {\n        background-color: #ffffff;\n        max-width: 468pt;\n        padding: 72pt 72pt 72pt 72pt;\n      }\n      .c0 {\n        color: #0e101a;\n        font-style: italic;\n      }\n      .c16 {\n        padding: 0;\n        margin: 0;\n      }\n      .c1 {\n        color: #0e101a;\n      }\n      .c7 {\n        font-weight: 700;\n      }\n      .c14 {\n        color: #000000;\n      }\n      .title {\n        padding-top: 0pt;\n        color: #000000;\n        font-size: 26pt;\n        padding-bottom: 3pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      .subtitle {\n        padding-top: 0pt;\n        color: #666666;\n        font-size: 15pt;\n        padding-bottom: 16pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      li {\n        color: #000000;\n        font-size: 11pt;\n        font-family: "Arial";\n      }\n      p {\n        margin: 0;\n        color: #000000;\n        font-size: 11pt;\n        font-family: "Arial";\n      }\n      h1 {\n        padding-top: 20pt;\n        color: #000000;\n        font-size: 20pt;\n        padding-bottom: 6pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h2 {\n        padding-top: 18pt;\n        color: #ffffff;\n        font-size: 16pt;\n        padding-bottom: 6pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h3 {\n        padding-top: 16pt;\n        color: #434343;\n        font-size: 14pt;\n        padding-bottom: 4pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h4 {\n        padding-top: 14pt;\n        color: #666666;\n        font-size: 12pt;\n        padding-bottom: 4pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h5 {\n        padding-top: 12pt;\n        color: #666666;\n        font-size: 11pt;\n        padding-bottom: 4pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n      h6 {\n        padding-top: 12pt;\n        color: #666666;\n        font-size: 11pt;\n        padding-bottom: 4pt;\n        font-family: "Arial";\n        line-height: 1.15;\n        page-break-after: avoid;\n        font-style: italic;\n        orphans: 2;\n        widows: 2;\n        text-align: left;\n      }\n    </style>\n    <style>\n      body.c5 {\n        background-color: #0e101a;\n      }\n      .title,\n      .subtitle,\n      h1,\n      h2,\n      h3,\n      h4,\n      h5,\n      h6,\n      p,\n      li,\n      span {\n        color: #ffffff !important;\n      }\n      .title {\n        font-size: 36pt !important;\n        text-align: center !important;\n      }\n      .subtitle {\n        font-size: 24pt !important;\n        text-align: center !important;\n      }\n      h1 {\n        font-size: 30pt !important;\n      }\n      h2 {\n        font-size: 24pt !important;\n      }\n      h3 {\n        font-size: 20pt !important;\n      }\n      h4 {\n        font-size: 18pt !important;\n      }\n      h5 {\n        font-size: 16pt !important;\n      }\n      h6 {\n        font-size: 14pt !important;\n      }\n      p {\n        margin-bottom: 1em;\n      }\n    </style>\n  </head>\n  <body class="c4 doc-content">\n    <h1 class="c13" id="h.49m0tfiw1rvi">\n      <span class="c6">Navigating our Neurons Highways</span>\n    </h1>\n    <h3 class="c11" id="h.xaqt23idce1n">\n      <span class="c9">Hiya, brilliant minds!</span>\n    </h3>\n    <p class="c2"><span class="c8"></span></p>\n    <p class="c3">\n      <span class="c1"\n        >I&#39;m excited to take you today on a delightful little expedition\n        into the fascinating world of data science and its collaboration with\n        neuroengineering. Trust me, it&#39;s less intimidating than it might\n        sound. In fact, it&#39;s quite a thrilling and self-motivating tale! Go\n        grab a cup of coffee or a glass of water, settle in, and let&#39;s chat\n        like old friends who recently reconnected while hiking an unknown\n        trail.</span\n      >\n    </p>\n    <p class="c2"><span class="c8"></span></p>\n    <p class="c3">\n      <span class="c1"\n        >Imagine our brain is a bustling city with neurons zipping around like\n        busy highway commuters. In this scenario, data science acts like the\n        GPS, an essential tool guiding these neurons to their destination\n        smoothly and efficiently. Yes, I am talking about a perfect blend of\n        computation and neuroscience: unraveling the intricate labyrinth of our\n        mind, one neuron at a time.</span\n      >\n    </p>\n    <p class="c2"><span class="c8"></span></p>\n    <p class="c3">\n      <span class="c1"\n        >Data science and engineering can sometimes be a heavy load to lift.\n        Still, with the right approach, we could unlock the secrets to the\n        universe, nestled right inside our heads! It&#39;s like embarking on the\n        most exhilarating treasure hunt, don&#39;t you agree?</span\n      >\n    </p>\n    \x3c!-- <p class="c3"><span class="c1">&nbsp;</span></p> --\x3e\n    <p class="c3">\n      <span class="c1"\n        >Here&#39;s the cherry on top: As future leaders in STEM-related fields,\n        we aren&#39;t just confined to textbooks and lab experiments nowadays.\n        Remember the first time you managed to compile a complex piece of code\n        or successfully debugged an intricate algorithm? It felt like hitting a\n        home run in the bottom of the ninth, didn&#39;t it? Well, imagine having\n        that feeling but on a cosmic scale. We have this golden opportunity to\n        be the pioneers, steering the ship into uncharted waters and making\n        discoveries that could revolutionize our understanding of the human\n        brain and, potentially, the entire world!</span\n      >\n    </p>\n    \x3c!-- <p class="c3"><span class="c1">&nbsp;</span></p> --\x3e\n    <p class="c3">\n      <span class="c10"\n        >&quot;Carlos, how do I get started on this exciting safari?&quot; I\n        thought you&#39;d never ask! As most things go in today&#39;s age, the\n        voyage begins with </span\n      ><span class="c0">embracing</span\n      ><span class="c1"\n        >&nbsp;the vibrant world of data science by getting comfortable with\n        machine learning algorithms. Meanwhile, we should also learn about the\n        intricate dance between data and neurons. Regardless, data science is\n        like learning a new language that allows us to communicate with the very\n        fabric of human existence.</span\n      >\n    </p>\n    \x3c!-- <p class="c3"><span class="c1">&nbsp;</span></p> --\x3e\n    <p class="c3">\n      <span class="c1"\n        >I won&#39;t lie; there might be more than a few bumps along the road.\n        But remember, every incredible quest begins with a single step. As we\n        venture further into this captivating realm, we&#39;ll find that the\n        lines between data science and neuroengineering blur; data science\n        weaves together a tapestry of profound and utterly beautiful knowledge\n        for its application.</span\n      >\n    </p>\n    <p class="c2"><span class="c8"></span></p>\n    <p class="c3">\n      <span class="c10"\n        >To learn more about data science tools and start applying them for your\n        specific situation using Python code, read my other blog post here: </span\n      ><span class="c7"\n        ><a\n          class="c5"\n          href="https://www.google.com/url?q=https://www.cargonriv.com/blog/data-science-tools-tutorials&amp;sa=D&amp;source=editors&amp;ust=1755856038449419&amp;usg=AOvVaw3q-e5Ke2WOQPTzXahjQSXI"\n          >cargonriv.com/blog/data-science-tools-tutorials</a\n        ></span\n      >\n    </p>\n    \x3c!-- <p class="c3"><span class="c1">&nbsp;</span></p> --\x3e\n    <p class="c3">\n      <span class="c1"\n        >So, my fellow adventurers, let&#39;s embrace this odyssey with open\n        hearts and curious minds as we stand at a new era&#39;s birth. We must\n        forge ahead, united in our quest to dig through the majestic landscape\n        between data science and neuroengineering. Who knows if future\n        discoveries in this integrative field may vastly redefine society&#39;s\n        state-of-the-art science and technology...</span\n      >\n    </p>\n    \x3c!-- <p class="c3"><span class="c1">&nbsp;</span></p> --\x3e\n    <p class="c3">\n      <span class="c10"\n        >Until our next adventure, keep dreaming, keep exploring, and remember,\n        the universe is overflowing with new opportunities, just waiting for us\n        to uncover them!</span\n      >\n    </p>\n  </body>\n</html>\n'}},hn=()=>{const{slug:t}=a(),i=e.useMemo(()=>t&&dn[t]||null,[t]);return i?n.jsx(pn,{title:i.title,description:i.description,html:i.html,canonical:window.location.href}):n.jsx(pn,{title:"Post not found | cargonriv.com",description:"The blog post you are looking for could not be found.",html:"<h1>Post not found</h1><p>We couldn't find this article. Please go back to the <a href='#/blog'>blog</a>.</p>",canonical:window.location.href})};export{hn as default};
